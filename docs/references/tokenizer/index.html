
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="http://127.0.0.1:8000/Users/ric/Documents/Projects/transformers-embedder/docs/references/tokenizer/">
      
      <link rel="icon" href="../../../../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.3.9">
    
    
      
        <title>Tokenizer - Transformers Embedder</title>
      
    
    
      <link rel="stylesheet" href="../../../../../../../../assets/stylesheets/main.1d29e8d0.min.css">
      
        
        <link rel="stylesheet" href="../../../../../../../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../../../../../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="yellow">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformers_embedder.tokenizer" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../../../../.." title="Transformers Embedder" class="md-header__button md-logo" aria-label="Transformers Embedder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Transformers Embedder
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Tokenizer
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="yellow"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="yellow"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/riccorl/transformers-embedder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../../../../.." title="Transformers Embedder" class="md-nav__button md-logo" aria-label="Transformers Embedder" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Transformers Embedder
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/riccorl/transformers-embedder" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          API References
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API References" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          API References
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_1" type="checkbox" id="__nav_1_1" >
      
      
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../../../../../references/__init__/">transformers_embedder</a>
          
            <label for="__nav_1_1">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="transformers_embedder" data-md-level="2">
        <label class="md-nav__title" for="__nav_1_1">
          <span class="md-nav__icon md-icon"></span>
          transformers_embedder
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../../references/embedder/" class="md-nav__link">
        embedder
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1_1_2" type="checkbox" id="__nav_1_1_2" >
      
      
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../../../../../../references/__init__/">modules</a>
          
            <label for="__nav_1_1_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="modules" data-md-level="3">
        <label class="md-nav__title" for="__nav_1_1_2">
          <span class="md-nav__icon md-icon"></span>
          modules
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../../references/encoder/" class="md-nav__link">
        encoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../../references/scalar_mix/" class="md-nav__link">
        scalar_mix
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../../references/tokenizer/" class="md-nav__link">
        tokenizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../../references/utils/" class="md-nav__link">
        utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer" class="md-nav__link">
    transformers_embedder.tokenizer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.ModelInputs" class="md-nav__link">
    ModelInputs
  </a>
  
    <nav class="md-nav" aria-label="ModelInputs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.ModelInputs.items" class="md-nav__link">
    items()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.ModelInputs.keys" class="md-nav__link">
    keys()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.ModelInputs.to" class="md-nav__link">
    to()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.ModelInputs.values" class="md-nav__link">
    values()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer" class="md-nav__link">
    Tokenizer
  </a>
  
    <nav class="md-nav" aria-label="Tokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer._clean_output" class="md-nav__link">
    _clean_output()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer._get_token_type_id" class="md-nav__link">
    _get_token_type_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer._type_checking" class="md-nav__link">
    _type_checking()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.add_padding_ops" class="md-nav__link">
    add_padding_ops()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.add_special_tokens" class="md-nav__link">
    add_special_tokens()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.add_to_tensor_inputs" class="md-nav__link">
    add_to_tensor_inputs()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.bos_token" class="md-nav__link">
    bos_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.bos_token_id" class="md-nav__link">
    bos_token_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.build_scatter_offsets" class="md-nav__link">
    build_scatter_offsets()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.build_sparse_offsets" class="md-nav__link">
    build_sparse_offsets()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.cls_token" class="md-nav__link">
    cls_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.cls_token_id" class="md-nav__link">
    cls_token_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.eos_token" class="md-nav__link">
    eos_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.eos_token_id" class="md-nav__link">
    eos_token_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.has_double_sep" class="md-nav__link">
    has_double_sep()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.has_starting_token" class="md-nav__link">
    has_starting_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.num_special_tokens" class="md-nav__link">
    num_special_tokens()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.pad_batch" class="md-nav__link">
    pad_batch()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.pad_sequence" class="md-nav__link">
    pad_sequence()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.pad_token" class="md-nav__link">
    pad_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.pad_token_id" class="md-nav__link">
    pad_token_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.sep_token" class="md-nav__link">
    sep_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.sep_token_id" class="md-nav__link">
    sep_token_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.to_tensor" class="md-nav__link">
    to_tensor()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.token_type_id" class="md-nav__link">
    token_type_id()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.unk_token" class="md-nav__link">
    unk_token()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformers_embedder.tokenizer.Tokenizer.unk_token_id" class="md-nav__link">
    unk_token_id()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/riccorl/transformers-embedder/edit/master/docs/transformers_embedder/tokenizer.py" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



  <h1>Tokenizer</h1>

<div class="doc doc-object doc-module">

<a id="transformers_embedder.tokenizer"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="transformers_embedder.tokenizer.ModelInputs" class="doc doc-heading">
          <code>ModelInputs</code>



</h2>


    <div class="doc doc-contents ">
        <p class="doc doc-class-bases">
          Bases: <code><span title="collections.UserDict">UserDict</span></code></p>


      <p>Model input dictionary wrapper.</p>


          <details class="quote">
            <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
            <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ModelInputs</span><span class="p">(</span><span class="n">UserDict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Model input dictionary wrapper.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;`ModelInputs` has no attribute `</span><span class="si">{</span><span class="n">item</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">item</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="s2">&quot;data&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A set-like object providing a view on D&#39;s keys.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;An object providing a view on D&#39;s values.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A set-like object providing a view on D&#39;s items.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Send all tensors values to device.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (:obj:`str` or :obj:`torch.device`): The device to put the tensors on.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :class:`tokenizers.ModelInputs`: The same instance of :class:`~tokenizers.ModelInputs`</span>
<span class="sd">            after modification.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Attempting to cast to another type, </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">. This is not supported.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
          </details>



  <div class="doc doc-children">












  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.ModelInputs.items" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">items</span><span class="p">()</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>A set-like object providing a view on D's items.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">items</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A set-like object providing a view on D&#39;s items.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.ModelInputs.keys" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">keys</span><span class="p">()</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>A set-like object providing a view on D's keys.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A set-like object providing a view on D&#39;s keys.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.ModelInputs.to" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Send all tensors values to device.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>device</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code> or :obj:<code>torch.device</code>): The device to put the tensors on.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="transformers_embedder.tokenizer.ModelInputs" href="../../../../../../../../../references/tokenizer/#transformers_embedder.tokenizer.ModelInputs">ModelInputs</a></code>
          </td>
          <td><p>class:<code>tokenizers.ModelInputs</code>: The same instance of :class:<code>~tokenizers.ModelInputs</code></p></td>
        </tr>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="transformers_embedder.tokenizer.ModelInputs" href="../../../../../../../../../references/tokenizer/#transformers_embedder.tokenizer.ModelInputs">ModelInputs</a></code>
          </td>
          <td><p>after modification.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Send all tensors values to device.</span>

<span class="sd">    Args:</span>
<span class="sd">        device (:obj:`str` or :obj:`torch.device`): The device to put the tensors on.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`tokenizers.ModelInputs`: The same instance of :class:`~tokenizers.ModelInputs`</span>
<span class="sd">        after modification.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Attempting to cast to another type, </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="si">}</span><span class="s2">. This is not supported.&quot;</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.ModelInputs.values" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">values</span><span class="p">()</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>An object providing a view on D's values.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;An object providing a view on D&#39;s values.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="transformers_embedder.tokenizer.Tokenizer" class="doc doc-heading">
          <code>Tokenizer</code>



</h2>


    <div class="doc doc-contents ">


      <p>A wrapper class for HuggingFace Tokenizer.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code>, :obj:<code>transformers.PreTrainedTokenizer</code>):
Language model name (or a transformer :obj:<code>PreTrainedTokenizer</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>return_sparse_offsets</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>True</code>):
If :obj:<code>True</code>, the sparse offsets of the tokens in the input text are returned. To reduce
memory usage, set this to :obj:<code>False</code> if you don't need them, e.g. you set the
<code>subword_pooling_strategy</code> to <code>scatter</code> in the <code>TransformersEmbedder</code> model.</p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
    </tbody>
  </table>


          <details class="quote">
            <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
            <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Tokenizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper class for HuggingFace Tokenizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (:obj:`str`, :obj:`transformers.PreTrainedTokenizer`):</span>
<span class="sd">            Language model name (or a transformer :obj:`PreTrainedTokenizer`.</span>
<span class="sd">        return_sparse_offsets (:obj:`bool`, optional, defaults to :obj:`True`):</span>
<span class="sd">            If :obj:`True`, the sparse offsets of the tokens in the input text are returned. To reduce</span>
<span class="sd">            memory usage, set this to :obj:`False` if you don&#39;t need them, e.g. you set the</span>
<span class="sd">            `subword_pooling_strategy` to `scatter` in the `TransformersEmbedder` model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tr</span><span class="o">.</span><span class="n">PreTrainedTokenizer</span><span class="p">],</span>
        <span class="n">return_sparse_offsets</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># init HuggingFace tokenizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>
            <span class="c1"># get config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span> <span class="o">=</span> <span class="n">model</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">tr</span><span class="o">.</span><span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">name_or_path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">return_sparse_offsets</span> <span class="o">=</span> <span class="n">return_sparse_offsets</span>

        <span class="c1"># padding stuff</span>
        <span class="c1"># default, batch length is model max length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">model_max_length</span>
        <span class="c1"># padding ops</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="c1"># keys that will be converted in tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Size of the full vocabulary with the added tokens.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
        <span class="n">text_pair</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">additional_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare the text in input for models that uses HuggingFace as embeddings.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`, :obj:`List[List[Word]]`, :obj:`List[Word]`):</span>
<span class="sd">                Text or batch of text to be encoded.</span>
<span class="sd">            text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`, :obj:`List[List[Word]]`, :obj:`List[Word]`):</span>
<span class="sd">                Text or batch of text to be encoded.</span>
<span class="sd">            padding (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">                If :obj:`True`, applies padding to the batch based on the maximum length of the batch.</span>
<span class="sd">            max_length (:obj:`int`, optional, defaults to :obj:`None`):</span>
<span class="sd">                If specified, truncates the input sequence to that value. Otherwise,</span>
<span class="sd">                uses the model max length.</span>
<span class="sd">            return_tensors (:obj:`bool`, optional, defaults to :obj:`None`):</span>
<span class="sd">                If :obj:`True`, the outputs is converted to :obj:`torch.Tensor`</span>
<span class="sd">            is_split_into_words (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">                If :obj:`True` and the input is a string, the input is split on spaces.</span>
<span class="sd">            additional_inputs (:obj:`Dict[str, Any]`, optional, defaults to :obj:`None`):</span>
<span class="sd">                Additional inputs to be passed to the model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`ModelInputs`: The inputs to the transformer model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># some checks before starting</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`return_tensors=&#39;tf&#39;` is not supported. Please use `return_tensors=&#39;pt&#39;` &quot;</span>
                <span class="s2">&quot;or `return_tensors=True`.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">return_tensors</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># check if input is batched or a single sample</span>
        <span class="n">is_batched</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
            <span class="ow">and</span> <span class="n">text</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">is_split_into_words</span><span class="p">)</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>  <span class="c1"># batch it</span>
            <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
            <span class="n">text_pair</span> <span class="o">=</span> <span class="p">[</span><span class="n">text_pair</span><span class="p">]</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># use huggingface tokenizer to encode the text</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span>
            <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># build the offsets used to pool the subwords</span>
        <span class="n">scatter_offsets</span><span class="p">,</span> <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_scatter_offsets</span><span class="p">(</span>
            <span class="n">model_inputs</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
            <span class="n">there_is_text_pair</span><span class="o">=</span><span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># convert to ModelInputs</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
        <span class="c1"># add the offsets to the model inputs</span>
        <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="s2">&quot;scatter_offsets&quot;</span><span class="p">:</span> <span class="n">scatter_offsets</span><span class="p">,</span> <span class="s2">&quot;sentence_lengths&quot;</span><span class="p">:</span> <span class="n">sentence_lengths</span><span class="p">}</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_sparse_offsets</span><span class="p">:</span>
            <span class="c1"># build the data used to pool the subwords when in sparse mode</span>
            <span class="n">bpe_info</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_sparse_offsets</span><span class="p">(</span>
                <span class="n">offsets</span><span class="o">=</span><span class="n">scatter_offsets</span><span class="p">,</span>
                <span class="n">bpe_mask</span><span class="o">=</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">words_per_sentence</span><span class="o">=</span><span class="n">sentence_lengths</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># add the bpe info to the model inputs</span>
            <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;sparse_offsets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="o">**</span><span class="n">bpe_info</span><span class="p">)</span>

        <span class="c1"># we also update the maximum batch length,</span>
        <span class="c1"># both for subword and word level</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">sentence_lengths</span><span class="p">)</span>

        <span class="c1"># check if we need to convert other stuff to tensors</span>
        <span class="k">if</span> <span class="n">additional_inputs</span><span class="p">:</span>
            <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">additional_inputs</span><span class="p">)</span>
            <span class="c1"># check if there is a padding strategy</span>
            <span class="k">if</span> <span class="n">padding</span><span class="p">:</span>
                <span class="n">missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">additional_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">missing_keys</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;There are no padding strategies for the following keys: </span><span class="si">{</span><span class="n">missing_keys</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="s2">&quot;Please add one with `tokenizer.add_padding_ops()`.&quot;</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad_batch</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>
            <span class="c1"># convert them to tensors</span>
            <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">model_inputs</span>

    <span class="k">def</span> <span class="nf">build_scatter_offsets</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_inputs</span><span class="p">:</span> <span class="n">BatchEncoding</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">there_is_text_pair</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build the offset tensor for the batch of inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_inputs (:obj:`BatchEncoding`):</span>
<span class="sd">                The inputs to the transformer model.</span>
<span class="sd">            return_tensors (:obj:`bool`, optional, defaults to :obj:`True`):</span>
<span class="sd">                If :obj:`True`, the outputs is converted to :obj:`torch.Tensor`</span>
<span class="sd">            there_is_text_pair (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">                If :obj:`True` `text_pair` is not None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List[List[int]]` or :obj:`torch.Tensor`: The offsets of the sub-tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># output data structure</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># model_inputs should be the output of the HuggingFace tokenizer</span>
        <span class="c1"># it contains the word offsets to reconstruct the original tokens from the</span>
        <span class="c1"># sub-tokens</span>
        <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)):</span>
            <span class="n">word_ids</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="p">)</span>
            <span class="c1"># it is slightly different from what we need, so here we make it compatible</span>
            <span class="c1"># with our subword pooling strategy</span>
            <span class="c1"># if the first token is a special token, we need to take it into account</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_starting_token</span><span class="p">:</span>
                <span class="n">word_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                    <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
                <span class="p">]</span>
            <span class="c1"># otherwise, we can just use word_ids as is</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_offsets</span> <span class="o">=</span> <span class="n">word_ids</span>
            <span class="c1"># here we retrieve the max offset for the sample, which will be used as SEP offset</span>
            <span class="c1"># and also as padding value for the offsets</span>
            <span class="n">sep_offset_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># replace first None occurrence with sep_offset</span>
            <span class="n">sep_index</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
            <span class="c1"># if there is a text pair, we need to adjust the offsets for the second text</span>
            <span class="k">if</span> <span class="n">there_is_text_pair</span><span class="p">:</span>
                <span class="c1"># some models have two SEP tokens in between the two texts</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_double_sep</span><span class="p">:</span>
                    <span class="n">sep_index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">sep_offset_value</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
                <span class="c1"># keep the first offsets as is, adjust the second ones</span>
                <span class="n">word_offsets</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="p">[:</span> <span class="n">sep_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                    <span class="n">w</span> <span class="o">+</span> <span class="n">sep_offset_value</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">w</span>
                    <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
                <span class="p">]</span>
                <span class="c1"># update again the sep_offset</span>
                <span class="n">sep_offset_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="c1"># replace first None occurrence with sep_offset, it should be the last SEP</span>
                <span class="n">sep_index</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
            <span class="c1"># keep track of the maximum offset for padding</span>
            <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_offsets</span><span class="p">)</span>
            <span class="n">sentence_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sep_offset_value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># replace remaining None occurrences with -1</span>
        <span class="c1"># the remaining None occurrences are the padding values</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="p">[[</span><span class="n">o</span> <span class="k">if</span> <span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">offset</span><span class="p">]</span> <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">offsets</span><span class="p">]</span>
        <span class="c1"># if return_tensor is True, we need to convert the offsets to tensors</span>
        <span class="k">if</span> <span class="n">return_tensors</span><span class="p">:</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">sentence_lengths</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">build_sparse_offsets</span><span class="p">(</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">bpe_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">words_per_sentence</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;Build tensors used as info for BPE pooling, starting from the BPE offsets.</span>

<span class="sd">        Args:</span>
<span class="sd">            offsets (:obj:`torch.Tensor` or :obj:`List[List[int]]`):</span>
<span class="sd">                The offsets to compute lengths from.</span>
<span class="sd">            bpe_mask (:obj:`torch.Tensor` or :obj:`List[List[int]]`):</span>
<span class="sd">                The attention mask at BPE level.</span>
<span class="sd">            words_per_sentence (:obj:`List[int]`):</span>
<span class="sd">                The sentence lengths, word-wise.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`Mapping[str, Any]`: Tensors used to construct the sparse one which pools the</span>
<span class="sd">            transformer encoding word-wise.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bpe_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">bpe_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">bpe_mask</span><span class="p">)</span>

        <span class="n">sentence_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">bpe_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># We want to build triplets as coordinates (document, word, bpe)</span>
        <span class="c1"># We start by creating the document index for each triplet</span>
        <span class="n">document_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">offsets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
            <span class="n">sentence_lengths</span>
        <span class="p">)</span>
        <span class="c1"># then the word indices</span>
        <span class="n">word_indices</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[</span><span class="n">offsets</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># lastly the bpe indices</span>
        <span class="n">max_range</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bpe_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">bpe_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">max_range</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bpe_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="n">unique_words</span><span class="p">,</span> <span class="n">word_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span>
            <span class="n">offsets</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">unpadded_word_lengths</span> <span class="o">=</span> <span class="n">word_lengths</span><span class="p">[</span><span class="n">unique_words</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># and their weight to be used as multiplication factors</span>
        <span class="n">bpe_weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">unpadded_word_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">unpadded_word_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">sparse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">document_indices</span><span class="p">,</span> <span class="n">word_indices</span><span class="p">,</span> <span class="n">bpe_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>

        <span class="n">bpe_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">bpe_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
                <span class="nb">max</span><span class="p">(</span><span class="n">words_per_sentence</span><span class="p">),</span>  <span class="c1"># max number of words per sentence</span>
                <span class="n">bpe_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># max bpe_number in batch wrt the sentence</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">sparse_indices</span><span class="o">=</span><span class="n">sparse_indices</span><span class="p">,</span>
            <span class="n">sparse_values</span><span class="o">=</span><span class="n">bpe_weights</span><span class="p">,</span>
            <span class="n">sparse_size</span><span class="o">=</span><span class="n">bpe_shape</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">pad_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelInputs</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]],</span>
        <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad the batch to its maximum length or to the specified :obj:`max_length`.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (:obj:`Dict[str, list]`):</span>
<span class="sd">                The batch to pad.</span>
<span class="sd">            max_length (:obj:`int`, optional):</span>
<span class="sd">                Override maximum length of the batch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`Dict[str, list]`: The padded batch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">max_length</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="n">max_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># get maximum len inside a batch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sentence_lengths&quot;</span><span class="p">])</span>

        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">:</span>
                <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>

        <span class="k">return</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pad_sequence</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sequence</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">value</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">length</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;subword&quot;</span><span class="p">,</span>
        <span class="n">pad_to_left</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pad the input to the specified length with the given value.</span>

<span class="sd">        Args:</span>
<span class="sd">            sequence (:obj:`List`, :obj:`torch.Tensor`):</span>
<span class="sd">                Element to pad, it can be either a :obj:`List` or a :obj:`torch.Tensor`.</span>
<span class="sd">            value (:obj:`int`):</span>
<span class="sd">                Value to use as padding.</span>
<span class="sd">            length (:obj:`int`, :obj:`str`, optional, defaults to :obj:`subword`):</span>
<span class="sd">                Length after pad.</span>
<span class="sd">            pad_to_left (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">                If :obj:`True`, pads to the left, right otherwise.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`List`, :obj:`torch.Tensor`: The padded sequence.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">length</span> <span class="o">==</span> <span class="s2">&quot;subword&quot;</span><span class="p">:</span>
            <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span>
        <span class="k">elif</span> <span class="n">length</span> <span class="o">==</span> <span class="s2">&quot;word&quot;</span><span class="p">:</span>
            <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;`length` must be an `int`, `subword` or `word`. Current value is `</span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Sequence tensor must be 1D. Current shape is `</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">`&quot;</span>
                <span class="p">)</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_to_left</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">padding</span><span class="p">,</span> <span class="n">sequence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">padding</span> <span class="o">+</span> <span class="n">sequence</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">sequence</span><span class="p">,</span> <span class="n">padding</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sequence</span> <span class="o">+</span> <span class="n">padding</span>

    <span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tr</span><span class="o">.</span><span class="n">AddedToken</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder.</span>
<span class="sd">        If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last</span>
<span class="sd">        index of the current vocabulary).</span>

<span class="sd">        Args:</span>
<span class="sd">            special_tokens_dict (:obj:`Dict`):</span>
<span class="sd">                The dictionary containing special tokens. Keys should be in</span>
<span class="sd">                the list of predefined special attributes: [``bos_token``, ``eos_token``,</span>
<span class="sd">                ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,</span>
<span class="sd">                ``additional_special_tokens``].</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Number of tokens added to the vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_padding_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add padding logic to custom fields.</span>
<span class="sd">        If the field is not in :obj:`self.to_tensor_inputs`, this method will add the key to it.</span>

<span class="sd">        Args:</span>
<span class="sd">            key (:obj:`str`):</span>
<span class="sd">                Name of the field in the tokenizer input.</span>
<span class="sd">            value (:obj:`Any`):</span>
<span class="sd">                Value to use for padding.</span>
<span class="sd">            length (:obj:`int`, :obj:`str`):</span>
<span class="sd">                Length to pad. It can be an :obj:`int`, or two string value</span>
<span class="sd">                - ``subword``: the element is padded to the batch max length relative to the subwords length</span>
<span class="sd">                - ``word``: the element is padded to the batch max length relative to the original word length</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_to_tensor_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Add these keys to the ones that will be converted in Tensors.</span>

<span class="sd">        Args:</span>
<span class="sd">            names (:obj:`str`, :obj:`set`):</span>
<span class="sd">                Name of the field (or fields) to convert to tensors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`set`: The set of keys that will be converted to tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">names</span> <span class="o">=</span> <span class="p">{</span><span class="n">names</span><span class="p">}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
            <span class="n">names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span> <span class="o">|=</span> <span class="n">names</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span>

    <span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelInputs</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in</span>
<span class="sd">        :obj:`self.to_tensor_inputs`. By default, only the standard model inputs are converted. Use</span>
<span class="sd">        :obj:`self.add_to_tensor_inputs` to add custom fields.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch (:obj:`List[dict]`, :obj:`dict`):</span>
<span class="sd">                Batch in input.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`ModelInputs`: The batch as tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># convert to tensor</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_clean_output</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Clean before output.</span>

<span class="sd">        Args:</span>
<span class="sd">            output (:obj`List[dict]`, :obj:`dict`):</span>
<span class="sd">                The output to clean.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`dict`: The cleaned output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># single sentence case, generalize</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">]</span>
        <span class="c1"># convert list to dict</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_token_type_id</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">PretrainedConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get token type id. Useful when dealing with models that don&#39;t accept 1 as type id.</span>
<span class="sd">        Args:</span>
<span class="sd">            config (:obj:`transformers.PretrainedConfig`):</span>
<span class="sd">                Transformer config.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: Correct token type id for that model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;type_vocab_size&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">type_vocab_size</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="mi">0</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_type_checking</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Checks type of the inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            text (:obj:`Any`):</span>
<span class="sd">                Text to check.</span>
<span class="sd">            text_pair (:obj:`Any`):</span>
<span class="sd">                Text pair to check.</span>

<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">is_type_correct</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Check if input type is correct, returning a boolean value.</span>

<span class="sd">            Args:</span>
<span class="sd">                text_to_check (:obj:`Any`):</span>
<span class="sd">                    text to check.</span>

<span class="sd">            Returns:</span>
<span class="sd">                :obj`bool`: :obj`True` if the type is correct.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">text_to_check</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                    <span class="ow">and</span> <span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                        <span class="ow">or</span> <span class="p">(</span>
                            <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                            <span class="ow">or</span> <span class="p">(</span>
                                <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                                <span class="ow">and</span> <span class="p">(</span>
                                    <span class="nb">len</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span>
                                    <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                                <span class="p">)</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_type_correct</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;text input must of type `str` (single example), `List[str]` (batch or single &quot;</span>
                <span class="s2">&quot;pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_type_correct</span><span class="p">(</span><span class="n">text_pair</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;text_pair input must be `str` (single example), `List[str]` (batch or single &quot;</span>
                <span class="s2">&quot;pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the number of special tokens the model needs.</span>
<span class="sd">        It assumes the input contains both sentences (:obj:`text` and :obj:`text_pair`).</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`int`: the number of special tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_DOUBLE_SEP</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">):</span>
            <span class="k">return</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span>
            <span class="p">(</span><span class="n">MODELS_WITH_DOUBLE_SEP</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">),</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="mi">3</span>
        <span class="k">return</span> <span class="mi">2</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">has_double_sep</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;True if tokenizer uses two SEP tokens.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_DOUBLE_SEP</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">has_starting_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;True if tokenizer uses a starting token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Padding token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Padding token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">pad_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Padding token id.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Unknown token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">unk_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Unknown token id.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">unk_token_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Classification token.</span>
<span class="sd">        To extract a summary of an input sequence leveraging self-attention along the</span>
<span class="sd">        full depth of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">cls_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Classification token id.</span>
<span class="sd">        To extract a summary of an input sequence leveraging self-attention along the</span>
<span class="sd">        full depth of the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Separation token, to separate context and query in an input sequence.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">sep_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Separation token id, to separate context and query in an input sequence.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">sep_token_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Beginning of sentence token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">bos_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Beginning of sentence token id.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End of sentence token.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;End of sentence token id.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
</code></pre></div></td></tr></table></div>
          </details>



  <div class="doc doc-children">















  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.__call__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">additional_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Prepare the text in input for models that uses HuggingFace as embeddings.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>text</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code>, :obj:<code>List[str]</code>, :obj:<code>List[List[str]]</code>, :obj:<code>List[List[Word]]</code>, :obj:<code>List[Word]</code>):
Text or batch of text to be encoded.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>text_pair</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code>, :obj:<code>List[str]</code>, :obj:<code>List[List[str]]</code>, :obj:<code>List[List[Word]]</code>, :obj:<code>List[Word]</code>):
Text or batch of text to be encoded.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>padding</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>False</code>):
If :obj:<code>True</code>, applies padding to the batch based on the maximum length of the batch.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>max_length</code></td>
          <td>
          </td>
          <td><p>obj:<code>int</code>, optional, defaults to :obj:<code>None</code>):
If specified, truncates the input sequence to that value. Otherwise,
uses the model max length.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>return_tensors</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>None</code>):
If :obj:<code>True</code>, the outputs is converted to :obj:<code>torch.Tensor</code></p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>is_split_into_words</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>False</code>):
If :obj:<code>True</code> and the input is a string, the input is split on spaces.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
        <tr>
          <td><code>additional_inputs</code></td>
          <td>
          </td>
          <td><p>obj:<code>Dict[str, Any]</code>, optional, defaults to :obj:<code>None</code>):
Additional inputs to be passed to the model.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="transformers_embedder.tokenizer.ModelInputs" href="../../../../../../../../../references/tokenizer/#transformers_embedder.tokenizer.ModelInputs">ModelInputs</a></code>
          </td>
          <td><p>obj:<code>ModelInputs</code>: The inputs to the transformer model.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
    <span class="n">text_pair</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">PaddingStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">truncation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">TruncationStrategy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_split_into_words</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">additional_inputs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prepare the text in input for models that uses HuggingFace as embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`, :obj:`List[List[Word]]`, :obj:`List[Word]`):</span>
<span class="sd">            Text or batch of text to be encoded.</span>
<span class="sd">        text_pair (:obj:`str`, :obj:`List[str]`, :obj:`List[List[str]]`, :obj:`List[List[Word]]`, :obj:`List[Word]`):</span>
<span class="sd">            Text or batch of text to be encoded.</span>
<span class="sd">        padding (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">            If :obj:`True`, applies padding to the batch based on the maximum length of the batch.</span>
<span class="sd">        max_length (:obj:`int`, optional, defaults to :obj:`None`):</span>
<span class="sd">            If specified, truncates the input sequence to that value. Otherwise,</span>
<span class="sd">            uses the model max length.</span>
<span class="sd">        return_tensors (:obj:`bool`, optional, defaults to :obj:`None`):</span>
<span class="sd">            If :obj:`True`, the outputs is converted to :obj:`torch.Tensor`</span>
<span class="sd">        is_split_into_words (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">            If :obj:`True` and the input is a string, the input is split on spaces.</span>
<span class="sd">        additional_inputs (:obj:`Dict[str, Any]`, optional, defaults to :obj:`None`):</span>
<span class="sd">            Additional inputs to be passed to the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`ModelInputs`: The inputs to the transformer model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># some checks before starting</span>
    <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;tf&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;`return_tensors=&#39;tf&#39;` is not supported. Please use `return_tensors=&#39;pt&#39;` &quot;</span>
            <span class="s2">&quot;or `return_tensors=True`.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">return_tensors</span> <span class="o">=</span> <span class="s2">&quot;pt&quot;</span>
    <span class="k">if</span> <span class="n">return_tensors</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">return_tensors</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># check if input is batched or a single sample</span>
    <span class="n">is_batched</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="ow">and</span> <span class="n">text</span>
        <span class="ow">and</span> <span class="p">(</span>
            <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">is_split_into_words</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_batched</span><span class="p">:</span>  <span class="c1"># batch it</span>
        <span class="n">text</span> <span class="o">=</span> <span class="p">[</span><span class="n">text</span><span class="p">]</span>
        <span class="n">text_pair</span> <span class="o">=</span> <span class="p">[</span><span class="n">text_pair</span><span class="p">]</span> <span class="k">if</span> <span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="c1"># use huggingface tokenizer to encode the text</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">(</span>
        <span class="n">text</span><span class="p">,</span>
        <span class="n">text_pair</span><span class="o">=</span><span class="n">text_pair</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="n">truncation</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">is_split_into_words</span><span class="o">=</span><span class="n">is_split_into_words</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># build the offsets used to pool the subwords</span>
    <span class="n">scatter_offsets</span><span class="p">,</span> <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_scatter_offsets</span><span class="p">(</span>
        <span class="n">model_inputs</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="n">return_tensors</span><span class="p">,</span>
        <span class="n">there_is_text_pair</span><span class="o">=</span><span class="n">text_pair</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># convert to ModelInputs</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
    <span class="c1"># add the offsets to the model inputs</span>
    <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span><span class="s2">&quot;scatter_offsets&quot;</span><span class="p">:</span> <span class="n">scatter_offsets</span><span class="p">,</span> <span class="s2">&quot;sentence_lengths&quot;</span><span class="p">:</span> <span class="n">sentence_lengths</span><span class="p">}</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_sparse_offsets</span><span class="p">:</span>
        <span class="c1"># build the data used to pool the subwords when in sparse mode</span>
        <span class="n">bpe_info</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">build_sparse_offsets</span><span class="p">(</span>
            <span class="n">offsets</span><span class="o">=</span><span class="n">scatter_offsets</span><span class="p">,</span>
            <span class="n">bpe_mask</span><span class="o">=</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">words_per_sentence</span><span class="o">=</span><span class="n">sentence_lengths</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># add the bpe info to the model inputs</span>
        <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;sparse_offsets&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="o">**</span><span class="n">bpe_info</span><span class="p">)</span>

    <span class="c1"># we also update the maximum batch length,</span>
    <span class="c1"># both for subword and word level</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">sentence_lengths</span><span class="p">)</span>

    <span class="c1"># check if we need to convert other stuff to tensors</span>
    <span class="k">if</span> <span class="n">additional_inputs</span><span class="p">:</span>
        <span class="n">model_inputs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">additional_inputs</span><span class="p">)</span>
        <span class="c1"># check if there is a padding strategy</span>
        <span class="k">if</span> <span class="n">padding</span><span class="p">:</span>
            <span class="n">missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">additional_inputs</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">missing_keys</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;There are no padding strategies for the following keys: </span><span class="si">{</span><span class="n">missing_keys</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Please add one with `tokenizer.add_padding_ops()`.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_batch</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>
        <span class="c1"># convert them to tensors</span>
        <span class="k">if</span> <span class="n">return_tensors</span> <span class="o">==</span> <span class="s2">&quot;pt&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model_inputs</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.__len__" class="doc doc-heading">
<code class="codehilite language-python"><span class="fm">__len__</span><span class="p">()</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Size of the full vocabulary with the added tokens.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Size of the full vocabulary with the added tokens.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer._clean_output" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">_clean_output</span><span class="p">(</span><span class="n">output</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Clean before output.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>output</code></td>
          <td>
          </td>
          <td><p>obj<code>List[dict]</code>, :obj:<code>dict</code>):
The output to clean.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Dict">Dict</span></code>
          </td>
          <td><p>obj:<code>dict</code>: The cleaned output.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">_clean_output</span><span class="p">(</span><span class="n">output</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clean before output.</span>

<span class="sd">    Args:</span>
<span class="sd">        output (:obj`List[dict]`, :obj:`dict`):</span>
<span class="sd">            The output to clean.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`dict`: The cleaned output.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># single sentence case, generalize</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="p">]</span>
    <span class="c1"># convert list to dict</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">output</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer._get_token_type_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">_get_token_type_id</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Get token type id. Useful when dealing with models that don't accept 1 as type id.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>config</code></td>
          <td>
          </td>
          <td><p>obj:<code>transformers.PretrainedConfig</code>):
Transformer config.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td><p>obj:<code>int</code>: Correct token type id for that model.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">_get_token_type_id</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">tr</span><span class="o">.</span><span class="n">PretrainedConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get token type id. Useful when dealing with models that don&#39;t accept 1 as type id.</span>
<span class="sd">    Args:</span>
<span class="sd">        config (:obj:`transformers.PretrainedConfig`):</span>
<span class="sd">            Transformer config.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`int`: Correct token type id for that model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;type_vocab_size&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">type_vocab_size</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer._type_checking" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">_type_checking</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Checks type of the inputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>text</code></td>
          <td>
          </td>
          <td><p>obj:<code>Any</code>):
Text to check.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>text_pair</code></td>
          <td>
          </td>
          <td><p>obj:<code>Any</code>):
Text pair to check.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">_type_checking</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">text_pair</span><span class="p">:</span> <span class="n">Any</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks type of the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (:obj:`Any`):</span>
<span class="sd">            Text to check.</span>
<span class="sd">        text_pair (:obj:`Any`):</span>
<span class="sd">            Text pair to check.</span>

<span class="sd">    Returns:</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">is_type_correct</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check if input type is correct, returning a boolean value.</span>

<span class="sd">        Args:</span>
<span class="sd">            text_to_check (:obj:`Any`):</span>
<span class="sd">                text to check.</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj`bool`: :obj`True` if the type is correct.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">text_to_check</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                <span class="ow">and</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
                    <span class="ow">or</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                        <span class="ow">or</span> <span class="p">(</span>
                            <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
                            <span class="ow">and</span> <span class="p">(</span>
                                <span class="nb">len</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span>
                                <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_to_check</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                            <span class="p">)</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_type_correct</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;text input must of type `str` (single example), `List[str]` (batch or single &quot;</span>
            <span class="s2">&quot;pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_type_correct</span><span class="p">(</span><span class="n">text_pair</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
            <span class="s2">&quot;text_pair input must be `str` (single example), `List[str]` (batch or single &quot;</span>
            <span class="s2">&quot;pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).&quot;</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.add_padding_ops" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">add_padding_ops</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Add padding logic to custom fields.
If the field is not in :obj:<code>self.to_tensor_inputs</code>, this method will add the key to it.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>key</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code>):
Name of the field in the tokenizer input.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>value</code></td>
          <td>
          </td>
          <td><p>obj:<code>Any</code>):
Value to use for padding.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>length</code></td>
          <td>
          </td>
          <td><p>obj:<code>int</code>, :obj:<code>str</code>):
Length to pad. It can be an :obj:<code>int</code>, or two string value
- <code>subword</code>: the element is padded to the batch max length relative to the subwords length
- <code>word</code>: the element is padded to the batch max length relative to the original word length</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_padding_ops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add padding logic to custom fields.</span>
<span class="sd">    If the field is not in :obj:`self.to_tensor_inputs`, this method will add the key to it.</span>

<span class="sd">    Args:</span>
<span class="sd">        key (:obj:`str`):</span>
<span class="sd">            Name of the field in the tokenizer input.</span>
<span class="sd">        value (:obj:`Any`):</span>
<span class="sd">            Value to use for padding.</span>
<span class="sd">        length (:obj:`int`, :obj:`str`):</span>
<span class="sd">            Length to pad. It can be an :obj:`int`, or two string value</span>
<span class="sd">            - ``subword``: the element is padded to the batch max length relative to the subwords length</span>
<span class="sd">            - ``word``: the element is padded to the batch max length relative to the original word length</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_sequence</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.add_special_tokens" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder.
If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last
index of the current vocabulary).</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>special_tokens_dict</code></td>
          <td>
          </td>
          <td><p>obj:<code>Dict</code>):
The dictionary containing special tokens. Keys should be in
the list of predefined special attributes: [<code>bos_token</code>, <code>eos_token</code>,
<code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>].</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td><p>obj:<code>int</code>: Number of tokens added to the vocabulary.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_special_tokens</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">special_tokens_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">tr</span><span class="o">.</span><span class="n">AddedToken</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder.</span>
<span class="sd">    If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last</span>
<span class="sd">    index of the current vocabulary).</span>

<span class="sd">    Args:</span>
<span class="sd">        special_tokens_dict (:obj:`Dict`):</span>
<span class="sd">            The dictionary containing special tokens. Keys should be in</span>
<span class="sd">            the list of predefined special attributes: [``bos_token``, ``eos_token``,</span>
<span class="sd">            ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,</span>
<span class="sd">            ``additional_special_tokens``].</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`int`: Number of tokens added to the vocabulary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">(</span><span class="n">special_tokens_dict</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.add_to_tensor_inputs" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">add_to_tensor_inputs</span><span class="p">(</span><span class="n">names</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Add these keys to the ones that will be converted in Tensors.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>names</code></td>
          <td>
          </td>
          <td><p>obj:<code>str</code>, :obj:<code>set</code>):
Name of the field (or fields) to convert to tensors.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Set">Set</span>[str]</code>
          </td>
          <td><p>obj:<code>set</code>: The set of keys that will be converted to tensors.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add_to_tensor_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add these keys to the ones that will be converted in Tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        names (:obj:`str`, :obj:`set`):</span>
<span class="sd">            Name of the field (or fields) to convert to tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`set`: The set of keys that will be converted to tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">names</span> <span class="o">=</span> <span class="p">{</span><span class="n">names</span><span class="p">}</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="nb">set</span><span class="p">):</span>
        <span class="n">names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span> <span class="o">|=</span> <span class="n">names</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.bos_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">bos_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Beginning of sentence token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">bos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Beginning of sentence token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">bos_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.bos_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">bos_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Beginning of sentence token id.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">bos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Beginning of sentence token id.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.build_scatter_offsets" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">build_scatter_offsets</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">there_is_text_pair</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Build the offset tensor for the batch of inputs.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>model_inputs</code></td>
          <td>
          </td>
          <td><p>obj:<code>BatchEncoding</code>):
The inputs to the transformer model.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>return_tensors</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>True</code>):
If :obj:<code>True</code>, the outputs is converted to :obj:<code>torch.Tensor</code></p></td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr>
          <td><code>there_is_text_pair</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>False</code>):
If :obj:<code>True</code> <code>text_pair</code> is not None.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Tuple">Tuple</span></code>
          </td>
          <td><p>obj:<code>List[List[int]]</code> or :obj:<code>torch.Tensor</code>: The offsets of the sub-tokens.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">build_scatter_offsets</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model_inputs</span><span class="p">:</span> <span class="n">BatchEncoding</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">there_is_text_pair</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build the offset tensor for the batch of inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        model_inputs (:obj:`BatchEncoding`):</span>
<span class="sd">            The inputs to the transformer model.</span>
<span class="sd">        return_tensors (:obj:`bool`, optional, defaults to :obj:`True`):</span>
<span class="sd">            If :obj:`True`, the outputs is converted to :obj:`torch.Tensor`</span>
<span class="sd">        there_is_text_pair (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">            If :obj:`True` `text_pair` is not None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`List[List[int]]` or :obj:`torch.Tensor`: The offsets of the sub-tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># output data structure</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sentence_lengths</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># model_inputs should be the output of the HuggingFace tokenizer</span>
    <span class="c1"># it contains the word offsets to reconstruct the original tokens from the</span>
    <span class="c1"># sub-tokens</span>
    <span class="k">for</span> <span class="n">batch_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)):</span>
        <span class="n">word_ids</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">batch_index</span><span class="p">)</span>
        <span class="c1"># it is slightly different from what we need, so here we make it compatible</span>
        <span class="c1"># with our subword pooling strategy</span>
        <span class="c1"># if the first token is a special token, we need to take it into account</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_starting_token</span><span class="p">:</span>
            <span class="n">word_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="p">]</span>
        <span class="c1"># otherwise, we can just use word_ids as is</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">word_offsets</span> <span class="o">=</span> <span class="n">word_ids</span>
        <span class="c1"># here we retrieve the max offset for the sample, which will be used as SEP offset</span>
        <span class="c1"># and also as padding value for the offsets</span>
        <span class="n">sep_offset_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="c1"># replace first None occurrence with sep_offset</span>
        <span class="n">sep_index</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
        <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
        <span class="c1"># if there is a text pair, we need to adjust the offsets for the second text</span>
        <span class="k">if</span> <span class="n">there_is_text_pair</span><span class="p">:</span>
            <span class="c1"># some models have two SEP tokens in between the two texts</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_double_sep</span><span class="p">:</span>
                <span class="n">sep_index</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">sep_offset_value</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
            <span class="c1"># keep the first offsets as is, adjust the second ones</span>
            <span class="n">word_offsets</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="p">[:</span> <span class="n">sep_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
                <span class="n">w</span> <span class="o">+</span> <span class="n">sep_offset_value</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">w</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
            <span class="p">]</span>
            <span class="c1"># update again the sep_offset</span>
            <span class="n">sep_offset_value</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">word_offsets</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="c1"># replace first None occurrence with sep_offset, it should be the last SEP</span>
            <span class="n">sep_index</span> <span class="o">=</span> <span class="n">word_offsets</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">word_offsets</span><span class="p">[</span><span class="n">sep_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">sep_offset_value</span>
        <span class="c1"># keep track of the maximum offset for padding</span>
        <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_offsets</span><span class="p">)</span>
        <span class="n">sentence_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sep_offset_value</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># replace remaining None occurrences with -1</span>
    <span class="c1"># the remaining None occurrences are the padding values</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="p">[[</span><span class="n">o</span> <span class="k">if</span> <span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">offset</span><span class="p">]</span> <span class="k">for</span> <span class="n">offset</span> <span class="ow">in</span> <span class="n">offsets</span><span class="p">]</span>
    <span class="c1"># if return_tensor is True, we need to convert the offsets to tensors</span>
    <span class="k">if</span> <span class="n">return_tensors</span><span class="p">:</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">sentence_lengths</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.build_sparse_offsets" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">build_sparse_offsets</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="n">bpe_mask</span><span class="p">,</span> <span class="n">words_per_sentence</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Build tensors used as info for BPE pooling, starting from the BPE offsets.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>offsets</code></td>
          <td>
          </td>
          <td><p>obj:<code>torch.Tensor</code> or :obj:<code>List[List[int]]</code>):
The offsets to compute lengths from.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>bpe_mask</code></td>
          <td>
          </td>
          <td><p>obj:<code>torch.Tensor</code> or :obj:<code>List[List[int]]</code>):
The attention mask at BPE level.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>words_per_sentence</code></td>
          <td>
          </td>
          <td><p>obj:<code>List[int]</code>):
The sentence lengths, word-wise.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Mapping">Mapping</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>obj:<code>Mapping[str, Any]</code>: Tensors used to construct the sparse one which pools the</p></td>
        </tr>
        <tr>
          <td>
                <code><span title="typing.Mapping">Mapping</span>[str, <span title="typing.Any">Any</span>]</code>
          </td>
          <td><p>transformer encoding word-wise.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">build_sparse_offsets</span><span class="p">(</span>
    <span class="n">offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">bpe_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">words_per_sentence</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Build tensors used as info for BPE pooling, starting from the BPE offsets.</span>

<span class="sd">    Args:</span>
<span class="sd">        offsets (:obj:`torch.Tensor` or :obj:`List[List[int]]`):</span>
<span class="sd">            The offsets to compute lengths from.</span>
<span class="sd">        bpe_mask (:obj:`torch.Tensor` or :obj:`List[List[int]]`):</span>
<span class="sd">            The attention mask at BPE level.</span>
<span class="sd">        words_per_sentence (:obj:`List[int]`):</span>
<span class="sd">            The sentence lengths, word-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`Mapping[str, Any]`: Tensors used to construct the sparse one which pools the</span>
<span class="sd">        transformer encoding word-wise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bpe_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">bpe_mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">bpe_mask</span><span class="p">)</span>

    <span class="n">sentence_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">bpe_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># We want to build triplets as coordinates (document, word, bpe)</span>
    <span class="c1"># We start by creating the document index for each triplet</span>
    <span class="n">document_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">offsets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span>
        <span class="n">sentence_lengths</span>
    <span class="p">)</span>
    <span class="c1"># then the word indices</span>
    <span class="n">word_indices</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[</span><span class="n">offsets</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># lastly the bpe indices</span>
    <span class="n">max_range</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">bpe_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">bpe_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">max_range</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bpe_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

    <span class="n">unique_words</span><span class="p">,</span> <span class="n">word_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span>
        <span class="n">offsets</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">unpadded_word_lengths</span> <span class="o">=</span> <span class="n">word_lengths</span><span class="p">[</span><span class="n">unique_words</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># and their weight to be used as multiplication factors</span>
    <span class="n">bpe_weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">unpadded_word_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">unpadded_word_lengths</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="p">)</span>

    <span class="n">sparse_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">document_indices</span><span class="p">,</span> <span class="n">word_indices</span><span class="p">,</span> <span class="n">bpe_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>

    <span class="n">bpe_shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">bpe_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
            <span class="nb">max</span><span class="p">(</span><span class="n">words_per_sentence</span><span class="p">),</span>  <span class="c1"># max number of words per sentence</span>
            <span class="n">bpe_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># max bpe_number in batch wrt the sentence</span>
        <span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">sparse_indices</span><span class="o">=</span><span class="n">sparse_indices</span><span class="p">,</span>
        <span class="n">sparse_values</span><span class="o">=</span><span class="n">bpe_weights</span><span class="p">,</span>
        <span class="n">sparse_size</span><span class="o">=</span><span class="n">bpe_shape</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.cls_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">cls_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Classification token.
To extract a summary of an input sequence leveraging self-attention along the
full depth of the model.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">cls_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Classification token.</span>
<span class="sd">    To extract a summary of an input sequence leveraging self-attention along the</span>
<span class="sd">    full depth of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">cls_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.cls_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">cls_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Classification token id.
To extract a summary of an input sequence leveraging self-attention along the
full depth of the model.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">cls_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Classification token id.</span>
<span class="sd">    To extract a summary of an input sequence leveraging self-attention along the</span>
<span class="sd">    full depth of the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.eos_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">eos_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>End of sentence token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">eos_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;End of sentence token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.eos_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">eos_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>End of sentence token id.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">eos_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;End of sentence token id.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.has_double_sep" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">has_double_sep</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>True if tokenizer uses two SEP tokens.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">has_double_sep</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True if tokenizer uses two SEP tokens.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_DOUBLE_SEP</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.has_starting_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">has_starting_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>True if tokenizer uses a starting token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">has_starting_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;True if tokenizer uses a starting token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.num_special_tokens" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">num_special_tokens</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Return the number of special tokens the model needs.
It assumes the input contains both sentences (:obj:<code>text</code> and :obj:<code>text_pair</code>).</p>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>int</code>
          </td>
          <td><p>obj:<code>int</code>: the number of special tokens.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">num_special_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the number of special tokens the model needs.</span>
<span class="sd">    It assumes the input contains both sentences (:obj:`text` and :obj:`text_pair`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`int`: the number of special tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_DOUBLE_SEP</span>
    <span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">4</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="p">,</span>
        <span class="p">(</span><span class="n">MODELS_WITH_DOUBLE_SEP</span><span class="p">,</span> <span class="n">MODELS_WITH_STARTING_TOKEN</span><span class="p">),</span>
    <span class="p">):</span>
        <span class="k">return</span> <span class="mi">3</span>
    <span class="k">return</span> <span class="mi">2</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.pad_batch" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">pad_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Pad the batch to its maximum length or to the specified :obj:<code>max_length</code>.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
          </td>
          <td><p>obj:<code>Dict[str, list]</code>):
The batch to pad.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>max_length</code></td>
          <td>
          </td>
          <td><p>obj:<code>int</code>, optional):
Override maximum length of the batch.</p></td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="transformers_embedder.tokenizer.ModelInputs" href="../../../../../../../../../references/tokenizer/#transformers_embedder.tokenizer.ModelInputs">ModelInputs</a></code>
          </td>
          <td><p>obj:<code>Dict[str, list]</code>: The padded batch.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pad_batch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelInputs</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]],</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad the batch to its maximum length or to the specified :obj:`max_length`.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (:obj:`Dict[str, list]`):</span>
<span class="sd">            The batch to pad.</span>
<span class="sd">        max_length (:obj:`int`, optional):</span>
<span class="sd">            Override maximum length of the batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`Dict[str, list]`: The padded batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">max_length</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="n">max_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># get maximum len inside a batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;sentence_lengths&quot;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">:</span>
            <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_ops</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span>

    <span class="k">return</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.pad_sequence" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="s1">&#39;subword&#39;</span><span class="p">,</span> <span class="n">pad_to_left</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Pad the input to the specified length with the given value.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>sequence</code></td>
          <td>
          </td>
          <td><p>obj:<code>List</code>, :obj:<code>torch.Tensor</code>):
Element to pad, it can be either a :obj:<code>List</code> or a :obj:<code>torch.Tensor</code>.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>value</code></td>
          <td>
          </td>
          <td><p>obj:<code>int</code>):
Value to use as padding.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>length</code></td>
          <td>
          </td>
          <td><p>obj:<code>int</code>, :obj:<code>str</code>, optional, defaults to :obj:<code>subword</code>):
Length after pad.</p></td>
          <td>
                <code>&#39;subword&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>pad_to_left</code></td>
          <td>
          </td>
          <td><p>obj:<code>bool</code>, optional, defaults to :obj:<code>False</code>):
If :obj:<code>True</code>, pads to the left, right otherwise.</p></td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="typing.List">List</span>, torch.<span title="torch.Tensor">Tensor</span>]</code>
          </td>
          <td><p>obj:<code>List</code>, :obj:<code>torch.Tensor</code>: The padded sequence.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">pad_sequence</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">sequence</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">value</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">length</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;subword&quot;</span><span class="p">,</span>
    <span class="n">pad_to_left</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pad the input to the specified length with the given value.</span>

<span class="sd">    Args:</span>
<span class="sd">        sequence (:obj:`List`, :obj:`torch.Tensor`):</span>
<span class="sd">            Element to pad, it can be either a :obj:`List` or a :obj:`torch.Tensor`.</span>
<span class="sd">        value (:obj:`int`):</span>
<span class="sd">            Value to use as padding.</span>
<span class="sd">        length (:obj:`int`, :obj:`str`, optional, defaults to :obj:`subword`):</span>
<span class="sd">            Length after pad.</span>
<span class="sd">        pad_to_left (:obj:`bool`, optional, defaults to :obj:`False`):</span>
<span class="sd">            If :obj:`True`, pads to the left, right otherwise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`List`, :obj:`torch.Tensor`: The padded sequence.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">length</span> <span class="o">==</span> <span class="s2">&quot;subword&quot;</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">subword_max_batch_len</span>
    <span class="k">elif</span> <span class="n">length</span> <span class="o">==</span> <span class="s2">&quot;word&quot;</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_max_batch_len</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;`length` must be an `int`, `subword` or `word`. Current value is `</span><span class="si">{</span><span class="n">length</span><span class="si">}</span><span class="s2">`&quot;</span>
            <span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="nb">abs</span><span class="p">(</span><span class="n">length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sequence tensor must be 1D. Current shape is `</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">`&quot;</span>
            <span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pad_to_left</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">padding</span><span class="p">,</span> <span class="n">sequence</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">padding</span> <span class="o">+</span> <span class="n">sequence</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">sequence</span><span class="p">,</span> <span class="n">padding</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sequence</span> <span class="o">+</span> <span class="n">padding</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.pad_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">pad_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Padding token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">pad_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Padding token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">pad_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.pad_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">pad_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Padding token id.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">pad_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Padding token id.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.sep_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sep_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Separation token, to separate context and query in an input sequence.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">sep_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Separation token, to separate context and query in an input sequence.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">sep_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.sep_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">sep_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Separation token id, to separate context and query in an input sequence.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">sep_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Separation token id, to separate context and query in an input sequence.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">sep_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.to_tensor" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">to_tensor</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span></code>


</h3>


    <div class="doc doc-contents ">

      <p>Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in
:obj:<code>self.to_tensor_inputs</code>. By default, only the standard model inputs are converted. Use
:obj:<code>self.add_to_tensor_inputs</code> to add custom fields.</p>

  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>batch</code></td>
          <td>
          </td>
          <td><p>obj:<code>List[dict]</code>, :obj:<code>dict</code>):
Batch in input.</p></td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>

  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code><a class="autorefs autorefs-internal" title="transformers_embedder.tokenizer.ModelInputs" href="../../../../../../../../../references/tokenizer/#transformers_embedder.tokenizer.ModelInputs">ModelInputs</a></code>
          </td>
          <td><p>obj:<code>ModelInputs</code>: The batch as tensor.</p></td>
        </tr>
    </tbody>
  </table>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ModelInputs</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> <span class="nb">dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelInputs</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in</span>
<span class="sd">    :obj:`self.to_tensor_inputs`. By default, only the standard model inputs are converted. Use</span>
<span class="sd">    :obj:`self.add_to_tensor_inputs` to add custom fields.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (:obj:`List[dict]`, :obj:`dict`):</span>
<span class="sd">            Batch in input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :obj:`ModelInputs`: The batch as tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># convert to tensor</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_tensor_inputs</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">v</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">ModelInputs</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.token_type_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">token_type_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Padding token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Padding token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_token_type_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.unk_token" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">unk_token</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Unknown token.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">unk_token</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unknown token.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">unk_token</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h3 id="transformers_embedder.tokenizer.Tokenizer.unk_token_id" class="doc doc-heading">
<code class="codehilite language-python"><span class="n">unk_token_id</span><span class="p">()</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h3>


    <div class="doc doc-contents ">

      <p>Unknown token id.</p>

        <details class="quote">
          <summary>Source code in <code>transformers_embedder/tokenizer.py</code></summary>
          <div class="codehilite"><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">unk_token_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unknown token id.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">huggingface_tokenizer</span><span class="o">.</span><span class="n">unk_token_id</span>
</code></pre></div></td></tr></table></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>






  </div>

    </div>

  </div>

              
            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://twitter.com/RiccrdoRicOrl" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://github.com/riccorl" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../../../../../../..", "features": ["search.suggest", "search.highlight"], "search": "../../../../../../../../assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../../../../../../assets/javascripts/bundle.6c7ad80a.min.js"></script>
      
    
  </body>
</html>