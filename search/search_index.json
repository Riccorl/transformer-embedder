{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"# Transformers Embedder  [![Open in Visual Studio Code](https://img.shields.io/badge/preview%20in-vscode.dev-blue)](https://github.dev/Riccorl/transformers-embedder) [![PyTorch](https://img.shields.io/badge/PyTorch-orange?logo=pytorch)](https://pytorch.org/) [![Transformers](https://img.shields.io/badge/4.34-\ud83e\udd17%20Transformers-6670ff)](https://huggingface.co/transformers/) [![Code style: black](https://img.shields.io/badge/code%20style-black-000000)](https://github.com/psf/black)  [![Upload to PyPi](https://github.com/Riccorl/transformers-embedder/actions/workflows/python-publish-pypi.yml/badge.svg)](https://github.com/Riccorl/transformers-embedder/actions/workflows/python-publish-pypi.yml) [![Upload to PyPi](https://github.com/Riccorl/transformers-embedder/actions/workflows/python-publish-conda.yml/badge.svg)](https://github.com/Riccorl/transformers-embedder/actions/workflows/python-publish-conda.yml) [![PyPi Version](https://img.shields.io/github/v/release/Riccorl/transformers-embedder)](https://github.com/Riccorl/transformers-embedder/releases) [![Anaconda-Server Badge](https://anaconda.org/riccorl/transformers-embedder/badges/version.svg)](https://anaconda.org/riccorl/transformers-embedder) [![DeepSource](https://deepsource.io/gh/Riccorl/transformers-embedder.svg/?label=active+issues)](https://deepsource.io/gh/Riccorl/transformers-embedder/?ref=repository-badge)   <p>A Word Level Transformer layer based on PyTorch and \ud83e\udd17 Transformers.</p>"},{"location":"#how-to-use","title":"How to use","text":"<p>Install the library from PyPI:</p> <pre><code>pip install transformers-embedder\n</code></pre> <p>or from Conda:</p> <pre><code>conda install -c riccorl transformers-embedder\n</code></pre> <p>It offers a PyTorch layer and a tokenizer that support almost every pretrained model from Huggingface  \ud83e\udd17Transformers library. Here is a quick example:</p> <pre><code>import transformers_embedder as tre\n\ntokenizer = tre.Tokenizer(\"bert-base-cased\")\n\nmodel = tre.TransformersEmbedder(\n    \"bert-base-cased\", subword_pooling_strategy=\"sparse\", layer_pooling_strategy=\"mean\"\n)\n\nexample = \"This is a sample sentence\"\ninputs = tokenizer(example, return_tensors=True)\n</code></pre> <pre><code>{\n   'input_ids': tensor([[ 101, 1188, 1110, 170, 6876, 5650,  102]]),\n   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]]),\n   'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]])\n   'scatter_offsets': tensor([[0, 1, 2, 3, 4, 5, 6]]),\n   'sparse_offsets': {\n        'sparse_indices': tensor(\n            [\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 1, 2, 3, 4, 5, 6],\n                [0, 1, 2, 3, 4, 5, 6]\n            ]\n        ), \n        'sparse_values': tensor([1., 1., 1., 1., 1., 1., 1.]), \n        'sparse_size': torch.Size([1, 7, 7])\n    },\n   'sentence_length': 7  # with special tokens included\n}\n</code></pre> <pre><code>outputs = model(**inputs)\n</code></pre> <pre><code># outputs.word_embeddings.shape[1:-1]       # remove [CLS] and [SEP]\ntorch.Size([1, 5, 768])\n# len(example)\n5\n</code></pre>"},{"location":"#info","title":"Info","text":"<p>One of the annoyance of using transformer-based models is that it is not trivial to compute word embeddings  from the sub-token embeddings they output. With this API it's as easy as using \ud83e\udd17Transformers to get  word-level embeddings from theoretically every transformer model it supports.</p>"},{"location":"#model","title":"Model","text":""},{"location":"#subword-pooling-strategy","title":"Subword Pooling Strategy","text":"<p>The <code>TransformersEmbedder</code> class offers 3 ways to get the embeddings:</p> <ul> <li><code>subword_pooling_strategy=\"sparse\"</code>: computes the mean of the embeddings of the sub-tokens of each word    (i.e. the embeddings of the sub-tokens are pooled together) using a sparse matrix multiplication. This    strategy is the default one.</li> <li><code>subword_pooling_strategy=\"scatter\"</code>: computes the mean of the embeddings of the sub-tokens of each word   using a scatter-gather operation. It is not deterministic, but it works with ONNX export.</li> <li><code>subword_pooling_strategy=\"none\"</code>: returns the raw output of the transformer model without sub-token pooling.</li> </ul> <p>Here a little feature table:</p> Pooling Deterministic ONNX Sparse :white_check_mark: :white_check_mark: :x: Scatter :white_check_mark: :x: :white_check_mark: None :x: :white_check_mark: :white_check_mark:"},{"location":"#layer-pooling-strategy","title":"Layer Pooling Strategy","text":"<p>There are also multiple type of outputs you can get using <code>layer_pooling_strategy</code> parameter:</p> <ul> <li><code>layer_pooling_strategy=\"last\"</code>: returns the last hidden state of the transformer model</li> <li><code>layer_pooling_strategy=\"concat\"</code>: returns the concatenation of the selected <code>output_layers</code> of the    transformer model</li> <li><code>layer_pooling_strategy=\"sum\"</code>: returns the sum of the selected <code>output_layers</code> of the transformer model</li> <li><code>layer_pooling_strategy=\"mean\"</code>: returns the average of the selected <code>output_layers</code> of the transformer model</li> <li><code>layer_pooling_strategy=\"scalar_mix\"</code>: returns the output of a parameterised scalar mixture layer of the     selected <code>output_layers</code> of the transformer model</li> </ul> <p>If you also want all the outputs from the HuggingFace model, you can set <code>return_all=True</code> to get them.</p> <pre><code>class TransformersEmbedder(torch.nn.Module):\n    def __init__(\n        self,\n        model: Union[str, tr.PreTrainedModel],\n        subword_pooling_strategy: str = \"sparse\",\n        layer_pooling_strategy: str = \"last\",\n        output_layers: Tuple[int] = (-4, -3, -2, -1),\n        fine_tune: bool = True,\n        return_all: bool = True,\n    )\n</code></pre>"},{"location":"#tokenizer","title":"Tokenizer","text":"<p>The <code>Tokenizer</code> class provides the <code>tokenize</code> method to preprocess the input for the <code>TransformersEmbedder</code>  layer. You can pass raw sentences, pre-tokenized sentences and sentences in batch. It will preprocess them  returning a dictionary with the inputs for the model. By passing <code>return_tensors=True</code> it will return the  inputs as <code>torch.Tensor</code>.</p> <p>By default, if you pass text (or batch) as strings, it uses the HuggingFace tokenizer to tokenize them.</p> <pre><code>text = \"This is a sample sentence\"\ntokenizer(text)\n\ntext = [\"This is a sample sentence\", \"This is another sample sentence\"]\ntokenizer(text)\n</code></pre> <p>You can pass a pre-tokenized sentence (or batch of sentences) by setting <code>is_split_into_words=True</code></p> <pre><code>text = [\"This\", \"is\", \"a\", \"sample\", \"sentence\"]\ntokenizer(text, is_split_into_words=True)\n\ntext = [\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"1\"],\n    [\"This\", \"is\", \"sample\", \"sentence\", \"2\"],\n]\ntokenizer(text, is_split_into_words=True)\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<p>First, initialize the tokenizer</p> <pre><code>import transformers_embedder as tre\n\ntokenizer = tre.Tokenizer(\"bert-base-cased\")\n</code></pre> <ul> <li>You can pass a single sentence as a string:</li> </ul> <pre><code>text = \"This is a sample sentence\"\ntokenizer(text)\n</code></pre> <pre><code>{\n{\n    'input_ids': [[101, 1188, 1110, 170, 6876, 5650, 102]],\n    'token_type_ids': [[0, 0, 0, 0, 0, 0, 0]],\n    'attention_mask': [[1, 1, 1, 1, 1, 1, 1]],\n    'scatter_offsets': [[0, 1, 2, 3, 4, 5, 6]],\n    'sparse_offsets': {\n        'sparse_indices': tensor(\n            [\n                [0, 0, 0, 0, 0, 0, 0],\n                [0, 1, 2, 3, 4, 5, 6],\n                [0, 1, 2, 3, 4, 5, 6]\n            ]\n        ),\n        'sparse_values': tensor([1., 1., 1., 1., 1., 1., 1.]),\n        'sparse_size': torch.Size([1, 7, 7])\n    },\n    'sentence_lengths': [7],\n}\n</code></pre> <ul> <li>A sentence pair</li> </ul> <pre><code>text = \"This is a sample sentence A\"\ntext_pair = \"This is a sample sentence B\"\ntokenizer(text, text_pair)\n</code></pre> <pre><code>{\n    'input_ids': [[101, 1188, 1110, 170, 6876, 5650, 138, 102, 1188, 1110, 170, 6876, 5650, 139, 102]],\n    'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]],\n    'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n    'scatter_offsets': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]],\n    'sparse_offsets': {\n        'sparse_indices': tensor(\n            [\n                [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0],\n                [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n                [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n            ]\n        ),\n        'sparse_values': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n        'sparse_size': torch.Size([1, 15, 15])\n    },\n    'sentence_lengths': [15],\n}\n</code></pre> <ul> <li>A batch of sentences or sentence pairs. Using <code>padding=True</code> and <code>return_tensors=True</code>, the tokenizer  returns the text ready for the model</li> </ul> <pre><code>batch = [\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"1\"],\n    [\"This\", \"is\", \"sample\", \"sentence\", \"2\"],\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"3\"],\n    # ...\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"n\", \"for\", \"batch\"],\n]\ntokenizer(batch, padding=True, return_tensors=True)\n\nbatch_pair = [\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"1\"],\n    [\"This\", \"is\", \"sample\", \"sentence\", \"pair\", \"2\"],\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"3\"],\n    # ...\n    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"n\", \"for\", \"batch\"],\n]\ntokenizer(batch, batch_pair, padding=True, return_tensors=True)\n</code></pre>"},{"location":"#custom-fields","title":"Custom fields","text":"<p>It is possible to add custom fields to the model input and tell the <code>tokenizer</code> how to pad them using  <code>add_padding_ops</code>. Start by initializing the tokenizer with the model name:</p> <pre><code>import transformers_embedder as tre\n\ntokenizer = tre.Tokenizer(\"bert-base-cased\")\n</code></pre> <p>Then add the custom fields to it:</p> <pre><code>custom_fields = {\n  \"custom_filed_1\": [\n    [0, 0, 0, 0, 1, 0, 0],\n    [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n  ]\n}\n</code></pre> <p>Now we can add the padding logic for our custom field <code>custom_filed_1</code>. <code>add_padding_ops</code> method takes in  input</p> <ul> <li><code>key</code>: name of the field in the tokenizer input</li> <li><code>value</code>: value to use for padding</li> <li><code>length</code>: length to pad. It can be an <code>int</code>, or two string value, <code>subword</code> in which the element is padded  to match the length of the subwords, and <code>word</code> where the element is padded relative to the length of the batch after the merge of the subwords.</li> </ul> <pre><code>tokenizer.add_padding_ops(\"custom_filed_1\", 0, \"word\")\n</code></pre> <p>Finally, we can tokenize the input with the custom field:</p> <pre><code>text = [\n    \"This is a sample sentence\",\n    \"This is another example sentence just make it longer, with a comma too!\"\n]\n\ntokenizer(text, padding=True, return_tensors=True, additional_inputs=custom_fields)\n</code></pre> <p>The inputs are ready for the model, including the custom filed.</p> <pre><code>&gt;&gt;&gt; inputs\n\n{\n    'input_ids': tensor(\n        [\n            [ 101, 1188, 1110, 170, 6876, 5650, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [ 101, 1188, 1110, 1330, 1859, 5650, 1198, 1294, 1122, 2039, 117, 1114, 170, 3254, 1918, 1315, 106, 102]\n        ]\n    ),\n    'token_type_ids': tensor(\n        [\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        ]\n    ), \n    'attention_mask': tensor(\n        [\n            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        ]\n    ),\n    'scatter_offsets': tensor(\n        [\n            [ 0, 1, 2, 3, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n            [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, 15, 16]\n        ]\n    ),\n    'sparse_offsets': {\n        'sparse_indices': tensor(\n            [\n                [ 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  1],\n                [ 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, 15, 16],\n                [ 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n            ]\n        ),\n        'sparse_values': tensor(\n            [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n            1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n            1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000]\n        ), \n        'sparse_size': torch.Size([2, 17, 18])\n    }\n    'sentence_lengths': [7, 17],\n}\n</code></pre>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Some code in the <code>TransformersEmbedder</code> class is taken from the PyTorch Scatter library. The pretrained models and the core of the tokenizer is from \ud83e\udd17 Transformers.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/__init__/","title":"init","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/","title":"Embedder","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder","title":"<code>TransformersEmbedder</code>","text":"<p>             Bases: <code>Module</code></p> <p>Transformer Embedder class.</p> <p>Word level embeddings from various transformer architectures from Huggingface Transformers API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`, `tr.PreTrainedModel`</code> <p>Transformer model to use (https://huggingface.co/models).</p> required <code>layer_pooling_strategy</code> <code>`str`, optional, defaults to `last`</code> <p>What output to get from the transformer model. The last hidden state (<code>last</code>), the concatenation of the selected hidden layers (<code>concat</code>), the sum of the selected hidden layers (<code>sum</code>), the average of the selected hidden layers (<code>mean</code>), or a scalar mixture of the selected hidden layers (<code>scalar_mix</code>).</p> <code>'last'</code> <code>subword_pooling_strategy</code> <code>`str`, optional, defaults to `sparse`</code> <p>What pooling strategy to use for the sub-word embeddings. Methods available are <code>sparse</code>, <code>scatter</code> and <code>none</code>. The <code>scatter</code> strategy is ONNX comptabile but uses <code>scatter_add_</code> that is not deterministic. The <code>sparse</code> strategy is deterministic but it is not comptabile with ONNX. When <code>subword_pooling_strategy</code> is <code>none</code>, the sub-word embeddings are not pooled.</p> <code>'scatter'</code> <code>output_layers</code> <code>`tuple`, `list`, `str`, optional, defaults to `(-4, -3, -2, -1)`</code> <p>Which hidden layers to get from the transformer model. If <code>output_layers</code> is <code>all</code>, all the hidden layers are returned. If <code>output_layers</code> is a tuple or a list, the hidden layers are selected according to the indexes in the tuple or list. If <code>output_layers</code> is a string, it must be <code>all</code>.</p> <code>(-4, -3, -2, -1)</code> <code>fine_tune</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the transformer model is fine-tuned during training.</p> <code>True</code> <code>return_all</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code>, returns all the outputs from the HuggingFace model.</p> <code>False</code> <code>from_pretrained</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the model is loaded from a pre-trained model, otherwise it is initialized with random weights. Usefull when you want to load a model from a specific checkpoint, without having to download the entire model.</p> <code>True</code> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>class TransformersEmbedder(torch.nn.Module):\n    \"\"\"\n    Transformer Embedder class.\n\n    Word level embeddings from various transformer architectures from Huggingface Transformers API.\n\n    Args:\n        model (`str`, `tr.PreTrainedModel`):\n            Transformer model to use (https://huggingface.co/models).\n        layer_pooling_strategy (`str`, optional, defaults to `last`):\n            What output to get from the transformer model. The last hidden state (``last``),\n            the concatenation of the selected hidden layers (``concat``), the sum of the selected hidden\n            layers (``sum``), the average of the selected hidden layers (``mean``), or a scalar mixture of\n            the selected hidden layers (``scalar_mix``).\n        subword_pooling_strategy (`str`, optional, defaults to `sparse`):\n            What pooling strategy to use for the sub-word embeddings. Methods available are ``sparse``,\n            ``scatter`` and ``none``. The ``scatter`` strategy is ONNX comptabile but uses ``scatter_add_``\n            that is not deterministic. The ``sparse`` strategy is deterministic but it is not comptabile\n            with ONNX. When ``subword_pooling_strategy`` is ``none``, the sub-word embeddings are not\n            pooled.\n        output_layers (`tuple`, `list`, `str`, optional, defaults to `(-4, -3, -2, -1)`):\n            Which hidden layers to get from the transformer model. If ``output_layers`` is ``all``,\n            all the hidden layers are returned. If ``output_layers`` is a tuple or a list, the hidden\n            layers are selected according to the indexes in the tuple or list. If ``output_layers`` is\n            a string, it must be ``all``.\n        fine_tune (`bool`, optional, defaults to `True`):\n            If ``True``, the transformer model is fine-tuned during training.\n        return_all (`bool`, optional, defaults to `False`):\n            If ``True``, returns all the outputs from the HuggingFace model.\n        from_pretrained (`bool`, optional, defaults to `True`):\n            If ``True``, the model is loaded from a pre-trained model, otherwise it is initialized with\n            random weights. Usefull when you want to load a model from a specific checkpoint, without\n            having to download the entire model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[str, tr.PreTrainedModel],\n        layer_pooling_strategy: str = \"last\",\n        subword_pooling_strategy: str = \"scatter\",\n        output_layers: Union[Sequence[int], str] = (-4, -3, -2, -1),\n        fine_tune: bool = True,\n        return_all: bool = False,\n        from_pretrained: bool = True,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__()\n        if isinstance(model, str):\n            self.config = tr.AutoConfig.from_pretrained(\n                model,\n                output_hidden_states=True,\n                output_attentions=True,\n                *args,\n                **kwargs,\n            )\n            if from_pretrained:\n                self.transformer_model = tr.AutoModel.from_pretrained(\n                    model, config=self.config, *args, **kwargs\n                )\n            else:\n                self.transformer_model = tr.AutoModel.from_config(\n                    self.config, *args, **kwargs\n                )\n        else:\n            self.transformer_model = model\n\n        # pooling strategy parameters\n        self.layer_pooling_strategy = layer_pooling_strategy\n        self.subword_pooling_strategy = subword_pooling_strategy\n\n        if output_layers == \"all\":\n            output_layers = tuple(\n                range(self.transformer_model.config.num_hidden_layers)\n            )\n\n        # check output_layers is well defined\n        if (\n            max(map(abs, output_layers))\n            &gt;= self.transformer_model.config.num_hidden_layers\n        ):\n            raise ValueError(\n                f\"`output_layers` parameter not valid, choose between 0 and \"\n                f\"{self.transformer_model.config.num_hidden_layers - 1}. \"\n                f\"Current value is `{output_layers}`\"\n            )\n        self.output_layers = output_layers\n\n        self._scalar_mix: Optional[ScalarMix] = None\n        if layer_pooling_strategy == \"scalar_mix\":\n            self._scalar_mix = ScalarMix(len(output_layers))\n\n        # check if return all transformer outputs\n        self.return_all = return_all\n\n        # if fine_tune is False, freeze all the transformer's parameters\n        if not fine_tune:\n            for param in self.transformer_model.parameters():\n                param.requires_grad = False\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        scatter_offsets: Optional[torch.Tensor] = None,\n        sparse_offsets: Optional[Mapping[str, Any]] = None,\n        **kwargs,\n    ) -&gt; TransformersEmbedderOutput:\n        \"\"\"\n        Forward method of the PyTorch module.\n\n        Args:\n            input_ids (`torch.Tensor`):\n                Input ids for the transformer model.\n            attention_mask (`torch.Tensor`, optional):\n                Attention mask for the transformer model.\n            token_type_ids (`torch.Tensor`, optional):\n                Token type ids for the transformer model.\n            scatter_offsets (`torch.Tensor`, optional):\n                Offsets of the sub-word, used to reconstruct the word embeddings using\n                the ``scatter`` method.\n            sparse_offsets (`Mapping[str, Any]`, optional):\n                Offsets of the sub-word, used to reconstruct the word embeddings using\n                the ``sparse`` method.\n\n        Returns:\n            `TransformersEmbedderOutput`:\n                Word level embeddings plus the output of the transformer model.\n        \"\"\"\n        # Some HuggingFace models don't have the\n        # token_type_ids parameter and fail even when it's given as None.\n        inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n        if token_type_ids is not None:\n            inputs[\"token_type_ids\"] = token_type_ids\n\n        # Shape: [batch_size, num_sub-words, embedding_size].\n        transformer_outputs = self.transformer_model(**inputs)\n        if self.layer_pooling_strategy == \"last\":\n            word_embeddings = transformer_outputs.last_hidden_state\n        elif self.layer_pooling_strategy == \"concat\":\n            word_embeddings = [\n                transformer_outputs.hidden_states[layer] for layer in self.output_layers\n            ]\n            word_embeddings = torch.cat(word_embeddings, dim=-1)\n        elif self.layer_pooling_strategy == \"sum\":\n            word_embeddings = [\n                transformer_outputs.hidden_states[layer] for layer in self.output_layers\n            ]\n            word_embeddings = torch.stack(word_embeddings, dim=0).sum(dim=0)\n        elif self.layer_pooling_strategy == \"mean\":\n            word_embeddings = [\n                transformer_outputs.hidden_states[layer] for layer in self.output_layers\n            ]\n            word_embeddings = torch.stack(word_embeddings, dim=0).mean(\n                dim=0, dtype=torch.float\n            )\n        elif self.layer_pooling_strategy == \"scalar_mix\":\n            word_embeddings = [\n                transformer_outputs.hidden_states[layer] for layer in self.output_layers\n            ]\n            word_embeddings = self._scalar_mix(word_embeddings)\n        else:\n            raise ValueError(\n                \"`layer_pooling_strategy` parameter not valid, choose between `last`, `concat`, \"\n                f\"`sum`, `mean` and `scalar_mix`. Current value `{self.layer_pooling_strategy}`\"\n            )\n\n        if (\n            self.subword_pooling_strategy != \"none\"\n            and scatter_offsets is None\n            and sparse_offsets is None\n        ):\n            raise ValueError(\n                \"`subword_pooling_strategy` is not `none` but neither `scatter_offsets` not `sparse_offsets` \"\n                \"were passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n                \"- Set `subword_pooling_strategy` to `none` or\\n\"\n                \"- Pass `scatter_offsets` to the model during forward or\\n\"\n                \"- Pass `sparse_offsets` to the model during forward.\"\n            )\n\n        if self.subword_pooling_strategy not in [\"none\", \"scatter\", \"sparse\"]:\n            raise ValueError(\n                \"`subword_pooling_strategy` parameter not valid, choose between `scatter`, `sparse`\"\n                f\" and `none`. Current value is `{self.subword_pooling_strategy}`.\"\n            )\n        if self.subword_pooling_strategy == \"scatter\":\n            if scatter_offsets is None:\n                raise ValueError(\n                    \"`subword_pooling_strategy` is `scatter` but `scatter_offsets` \"\n                    \"were not passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n                    \"- Set `subword_pooling_strategy` to `none` or\\n\"\n                    \"- Pass `scatter_offsets` to the model during forward.\"\n                )\n            word_embeddings = self.merge_scatter(\n                word_embeddings, indices=scatter_offsets\n            )\n        if self.subword_pooling_strategy == \"sparse\":\n            if sparse_offsets is None:\n                raise ValueError(\n                    \"`subword_pooling_strategy` is `sparse` but `sparse_offsets` \"\n                    \"were not passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n                    \"- Set `subword_pooling_strategy` to `none` or\\n\"\n                    \"- Pass `sparse_offsets` to the model during forward.\"\n                )\n            word_embeddings = self.merge_sparse(word_embeddings, sparse_offsets)\n\n        if self.return_all:\n            return TransformersEmbedderOutput(\n                word_embeddings=word_embeddings,\n                last_hidden_state=transformer_outputs.last_hidden_state,\n                hidden_states=transformer_outputs.hidden_states,\n                pooler_output=transformer_outputs.pooler_output\n                if hasattr(transformer_outputs, \"pooler_output\")\n                else None,\n                attentions=transformer_outputs.attentions,\n            )\n        return TransformersEmbedderOutput(word_embeddings=word_embeddings)\n\n    @staticmethod\n    def merge_scatter(embeddings: torch.Tensor, indices: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Minimal version of ``scatter_mean``, from `pytorch_scatter\n        &lt;https://github.com/rusty1s/pytorch_scatter/&gt;`_\n        library, that is compatible for ONNX but works only for our case.\n        It is used to compute word level embeddings from the transformer output.\n\n        Args:\n            embeddings (`torch.Tensor`):\n                The embeddings tensor.\n            indices (`torch.Tensor`):\n                The sub-word indices.\n\n        Returns:\n            `torch.Tensor`\n        \"\"\"\n\n        def broadcast(src: torch.Tensor, other: torch.Tensor):\n            \"\"\"\n            Broadcast ``src`` to match the shape of ``other``.\n\n            Args:\n                src (`torch.Tensor`):\n                    The tensor to broadcast.\n                other (`torch.Tensor`):\n                    The tensor to match the shape of.\n\n            Returns:\n                `torch.Tensor`: The broadcasted tensor.\n            \"\"\"\n            for _ in range(src.dim(), other.dim()):\n                src = src.unsqueeze(-1)\n            src = src.expand_as(other)\n            return src\n\n        def scatter_sum(src: torch.Tensor, index: torch.Tensor) -&gt; torch.Tensor:\n            \"\"\"\n            Sums the elements in ``src`` that have the same indices as in ``index``.\n\n            Args:\n                src (`torch.Tensor`):\n                    The tensor to sum.\n                index (`torch.Tensor`):\n                    The indices to sum.\n\n            Returns:\n                `torch.Tensor`: The summed tensor.\n            \"\"\"\n            index = broadcast(index, src)\n            size = list(src.size())\n            size[1] = index.max() + 1\n            out = torch.zeros(size, dtype=src.dtype, device=src.device)\n            return out.scatter_add_(1, index, src)\n\n        # replace padding indices with the maximum value inside the batch\n        indices[indices == -1] = torch.max(indices)\n        merged = scatter_sum(embeddings, indices)\n        ones = torch.ones(\n            indices.size(), dtype=embeddings.dtype, device=embeddings.device\n        )\n        count = scatter_sum(ones, indices)\n        count.clamp_(1)\n        count = broadcast(count, merged)\n        merged.true_divide_(count)\n        return merged\n\n    @staticmethod\n    def merge_sparse(\n        embeddings: torch.Tensor, bpe_info: Optional[Mapping[str, Any]]\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Merges the subword embeddings into a single tensor, using sparse indices.\n\n        Args:\n            embeddings (`torch.Tensor`):\n                The embeddings tensor.\n            bpe_info (`Mapping[str, Any]`, `optional`):\n                The BPE info.\n\n        Returns:\n            `torch.Tensor`: The merged embeddings.\n        \"\"\"\n        # it is constructed here and not in the tokenizer/collate because pin_memory is not sparse-compatible\n        bpe_weights = torch.sparse_coo_tensor(\n            indices=bpe_info[\"sparse_indices\"],\n            values=bpe_info[\"sparse_values\"],\n            size=bpe_info[\"sparse_size\"],\n        )\n        # (sentence, word, bpe) x (sentence, bpe, transformer_dim) -&gt; (sentence, word, transformer_dim)\n        merged = torch.bmm(bpe_weights.to_dense(), embeddings)\n        return merged\n\n    def resize_token_embeddings(\n        self, new_num_tokens: Optional[int] = None\n    ) -&gt; torch.nn.Embedding:\n        \"\"\"\n        Resizes input token embeddings' matrix of the model if `new_num_tokens != config.vocab_size`.\n\n        Args:\n            new_num_tokens (`int`):\n                The number of new tokens in the embedding matrix.\n\n        Returns:\n            `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n        \"\"\"\n        return self.transformer_model.resize_token_embeddings(new_num_tokens)\n\n    def save_pretrained(self, save_directory: Union[str, Path]):\n        \"\"\"\n        Save a model and its configuration file to a directory.\n\n        Args:\n            save_directory (`str`, `Path`):\n                Directory to which to save.\n        \"\"\"\n        self.transformer_model.save_pretrained(save_directory)\n\n    @property\n    def hidden_size(self) -&gt; int:\n        \"\"\"\n        Returns the hidden size of TransformersEmbedder.\n\n        Returns:\n            `int`: Hidden size of ``self.transformer_model``.\n        \"\"\"\n        multiplier = (\n            len(self.output_layers) if self.layer_pooling_strategy == \"concat\" else 1\n        )\n        return self.transformer_model.config.hidden_size * multiplier\n\n    @property\n    def transformer_hidden_size(self) -&gt; int:\n        \"\"\"\n        Returns the hidden size of the inner transformer.\n\n        Returns:\n            `int`: Hidden size of ``self.transformer_model``.\n        \"\"\"\n        multiplier = (\n            len(self.output_layers) if self.layer_pooling_strategy == \"concat\" else 1\n        )\n        return self.transformer_model.config.hidden_size * multiplier\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.hidden_size","title":"<code>hidden_size: int</code>  <code>property</code>","text":"<p>Returns the hidden size of TransformersEmbedder.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>int</code>: Hidden size of <code>self.transformer_model</code>.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.transformer_hidden_size","title":"<code>transformer_hidden_size: int</code>  <code>property</code>","text":"<p>Returns the hidden size of the inner transformer.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>int</code>: Hidden size of <code>self.transformer_model</code>.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.forward","title":"<code>forward(input_ids, attention_mask=None, token_type_ids=None, scatter_offsets=None, sparse_offsets=None, **kwargs)</code>","text":"<p>Forward method of the PyTorch module.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>`torch.Tensor`</code> <p>Input ids for the transformer model.</p> required <code>attention_mask</code> <code>`torch.Tensor`</code> <p>Attention mask for the transformer model.</p> <code>None</code> <code>token_type_ids</code> <code>`torch.Tensor`</code> <p>Token type ids for the transformer model.</p> <code>None</code> <code>scatter_offsets</code> <code>`torch.Tensor`</code> <p>Offsets of the sub-word, used to reconstruct the word embeddings using the <code>scatter</code> method.</p> <code>None</code> <code>sparse_offsets</code> <code>`Mapping[str, Any]`</code> <p>Offsets of the sub-word, used to reconstruct the word embeddings using the <code>sparse</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <code>TransformersEmbedderOutput</code> <p><code>TransformersEmbedderOutput</code>: Word level embeddings plus the output of the transformer model.</p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    scatter_offsets: Optional[torch.Tensor] = None,\n    sparse_offsets: Optional[Mapping[str, Any]] = None,\n    **kwargs,\n) -&gt; TransformersEmbedderOutput:\n    \"\"\"\n    Forward method of the PyTorch module.\n\n    Args:\n        input_ids (`torch.Tensor`):\n            Input ids for the transformer model.\n        attention_mask (`torch.Tensor`, optional):\n            Attention mask for the transformer model.\n        token_type_ids (`torch.Tensor`, optional):\n            Token type ids for the transformer model.\n        scatter_offsets (`torch.Tensor`, optional):\n            Offsets of the sub-word, used to reconstruct the word embeddings using\n            the ``scatter`` method.\n        sparse_offsets (`Mapping[str, Any]`, optional):\n            Offsets of the sub-word, used to reconstruct the word embeddings using\n            the ``sparse`` method.\n\n    Returns:\n        `TransformersEmbedderOutput`:\n            Word level embeddings plus the output of the transformer model.\n    \"\"\"\n    # Some HuggingFace models don't have the\n    # token_type_ids parameter and fail even when it's given as None.\n    inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n    if token_type_ids is not None:\n        inputs[\"token_type_ids\"] = token_type_ids\n\n    # Shape: [batch_size, num_sub-words, embedding_size].\n    transformer_outputs = self.transformer_model(**inputs)\n    if self.layer_pooling_strategy == \"last\":\n        word_embeddings = transformer_outputs.last_hidden_state\n    elif self.layer_pooling_strategy == \"concat\":\n        word_embeddings = [\n            transformer_outputs.hidden_states[layer] for layer in self.output_layers\n        ]\n        word_embeddings = torch.cat(word_embeddings, dim=-1)\n    elif self.layer_pooling_strategy == \"sum\":\n        word_embeddings = [\n            transformer_outputs.hidden_states[layer] for layer in self.output_layers\n        ]\n        word_embeddings = torch.stack(word_embeddings, dim=0).sum(dim=0)\n    elif self.layer_pooling_strategy == \"mean\":\n        word_embeddings = [\n            transformer_outputs.hidden_states[layer] for layer in self.output_layers\n        ]\n        word_embeddings = torch.stack(word_embeddings, dim=0).mean(\n            dim=0, dtype=torch.float\n        )\n    elif self.layer_pooling_strategy == \"scalar_mix\":\n        word_embeddings = [\n            transformer_outputs.hidden_states[layer] for layer in self.output_layers\n        ]\n        word_embeddings = self._scalar_mix(word_embeddings)\n    else:\n        raise ValueError(\n            \"`layer_pooling_strategy` parameter not valid, choose between `last`, `concat`, \"\n            f\"`sum`, `mean` and `scalar_mix`. Current value `{self.layer_pooling_strategy}`\"\n        )\n\n    if (\n        self.subword_pooling_strategy != \"none\"\n        and scatter_offsets is None\n        and sparse_offsets is None\n    ):\n        raise ValueError(\n            \"`subword_pooling_strategy` is not `none` but neither `scatter_offsets` not `sparse_offsets` \"\n            \"were passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n            \"- Set `subword_pooling_strategy` to `none` or\\n\"\n            \"- Pass `scatter_offsets` to the model during forward or\\n\"\n            \"- Pass `sparse_offsets` to the model during forward.\"\n        )\n\n    if self.subword_pooling_strategy not in [\"none\", \"scatter\", \"sparse\"]:\n        raise ValueError(\n            \"`subword_pooling_strategy` parameter not valid, choose between `scatter`, `sparse`\"\n            f\" and `none`. Current value is `{self.subword_pooling_strategy}`.\"\n        )\n    if self.subword_pooling_strategy == \"scatter\":\n        if scatter_offsets is None:\n            raise ValueError(\n                \"`subword_pooling_strategy` is `scatter` but `scatter_offsets` \"\n                \"were not passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n                \"- Set `subword_pooling_strategy` to `none` or\\n\"\n                \"- Pass `scatter_offsets` to the model during forward.\"\n            )\n        word_embeddings = self.merge_scatter(\n            word_embeddings, indices=scatter_offsets\n        )\n    if self.subword_pooling_strategy == \"sparse\":\n        if sparse_offsets is None:\n            raise ValueError(\n                \"`subword_pooling_strategy` is `sparse` but `sparse_offsets` \"\n                \"were not passed to the model. Cannot compute word embeddings.\\nTo solve:\\n\"\n                \"- Set `subword_pooling_strategy` to `none` or\\n\"\n                \"- Pass `sparse_offsets` to the model during forward.\"\n            )\n        word_embeddings = self.merge_sparse(word_embeddings, sparse_offsets)\n\n    if self.return_all:\n        return TransformersEmbedderOutput(\n            word_embeddings=word_embeddings,\n            last_hidden_state=transformer_outputs.last_hidden_state,\n            hidden_states=transformer_outputs.hidden_states,\n            pooler_output=transformer_outputs.pooler_output\n            if hasattr(transformer_outputs, \"pooler_output\")\n            else None,\n            attentions=transformer_outputs.attentions,\n        )\n    return TransformersEmbedderOutput(word_embeddings=word_embeddings)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.merge_scatter","title":"<code>merge_scatter(embeddings, indices)</code>  <code>staticmethod</code>","text":"<p>Minimal version of <code>scatter_mean</code>, from <code>pytorch_scatter &lt;https://github.com/rusty1s/pytorch_scatter/&gt;</code>_ library, that is compatible for ONNX but works only for our case. It is used to compute word level embeddings from the transformer output.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>`torch.Tensor`</code> <p>The embeddings tensor.</p> required <code>indices</code> <code>`torch.Tensor`</code> <p>The sub-word indices.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code></p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>@staticmethod\ndef merge_scatter(embeddings: torch.Tensor, indices: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Minimal version of ``scatter_mean``, from `pytorch_scatter\n    &lt;https://github.com/rusty1s/pytorch_scatter/&gt;`_\n    library, that is compatible for ONNX but works only for our case.\n    It is used to compute word level embeddings from the transformer output.\n\n    Args:\n        embeddings (`torch.Tensor`):\n            The embeddings tensor.\n        indices (`torch.Tensor`):\n            The sub-word indices.\n\n    Returns:\n        `torch.Tensor`\n    \"\"\"\n\n    def broadcast(src: torch.Tensor, other: torch.Tensor):\n        \"\"\"\n        Broadcast ``src`` to match the shape of ``other``.\n\n        Args:\n            src (`torch.Tensor`):\n                The tensor to broadcast.\n            other (`torch.Tensor`):\n                The tensor to match the shape of.\n\n        Returns:\n            `torch.Tensor`: The broadcasted tensor.\n        \"\"\"\n        for _ in range(src.dim(), other.dim()):\n            src = src.unsqueeze(-1)\n        src = src.expand_as(other)\n        return src\n\n    def scatter_sum(src: torch.Tensor, index: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Sums the elements in ``src`` that have the same indices as in ``index``.\n\n        Args:\n            src (`torch.Tensor`):\n                The tensor to sum.\n            index (`torch.Tensor`):\n                The indices to sum.\n\n        Returns:\n            `torch.Tensor`: The summed tensor.\n        \"\"\"\n        index = broadcast(index, src)\n        size = list(src.size())\n        size[1] = index.max() + 1\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n        return out.scatter_add_(1, index, src)\n\n    # replace padding indices with the maximum value inside the batch\n    indices[indices == -1] = torch.max(indices)\n    merged = scatter_sum(embeddings, indices)\n    ones = torch.ones(\n        indices.size(), dtype=embeddings.dtype, device=embeddings.device\n    )\n    count = scatter_sum(ones, indices)\n    count.clamp_(1)\n    count = broadcast(count, merged)\n    merged.true_divide_(count)\n    return merged\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.merge_sparse","title":"<code>merge_sparse(embeddings, bpe_info)</code>  <code>staticmethod</code>","text":"<p>Merges the subword embeddings into a single tensor, using sparse indices.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>`torch.Tensor`</code> <p>The embeddings tensor.</p> required <code>bpe_info</code> <code>`Mapping[str, Any]`, `optional`</code> <p>The BPE info.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: The merged embeddings.</p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>@staticmethod\ndef merge_sparse(\n    embeddings: torch.Tensor, bpe_info: Optional[Mapping[str, Any]]\n) -&gt; torch.Tensor:\n    \"\"\"\n    Merges the subword embeddings into a single tensor, using sparse indices.\n\n    Args:\n        embeddings (`torch.Tensor`):\n            The embeddings tensor.\n        bpe_info (`Mapping[str, Any]`, `optional`):\n            The BPE info.\n\n    Returns:\n        `torch.Tensor`: The merged embeddings.\n    \"\"\"\n    # it is constructed here and not in the tokenizer/collate because pin_memory is not sparse-compatible\n    bpe_weights = torch.sparse_coo_tensor(\n        indices=bpe_info[\"sparse_indices\"],\n        values=bpe_info[\"sparse_values\"],\n        size=bpe_info[\"sparse_size\"],\n    )\n    # (sentence, word, bpe) x (sentence, bpe, transformer_dim) -&gt; (sentence, word, transformer_dim)\n    merged = torch.bmm(bpe_weights.to_dense(), embeddings)\n    return merged\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.resize_token_embeddings","title":"<code>resize_token_embeddings(new_num_tokens=None)</code>","text":"<p>Resizes input token embeddings' matrix of the model if <code>new_num_tokens != config.vocab_size</code>.</p> <p>Parameters:</p> Name Type Description Default <code>new_num_tokens</code> <code>`int`</code> <p>The number of new tokens in the embedding matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>Embedding</code> <p><code>torch.nn.Embedding</code>: Pointer to the input tokens Embeddings Module of the model.</p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>def resize_token_embeddings(\n    self, new_num_tokens: Optional[int] = None\n) -&gt; torch.nn.Embedding:\n    \"\"\"\n    Resizes input token embeddings' matrix of the model if `new_num_tokens != config.vocab_size`.\n\n    Args:\n        new_num_tokens (`int`):\n            The number of new tokens in the embedding matrix.\n\n    Returns:\n        `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n    \"\"\"\n    return self.transformer_model.resize_token_embeddings(new_num_tokens)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedder.save_pretrained","title":"<code>save_pretrained(save_directory)</code>","text":"<p>Save a model and its configuration file to a directory.</p> <p>Parameters:</p> Name Type Description Default <code>save_directory</code> <code>`str`, `Path`</code> <p>Directory to which to save.</p> required Source code in <code>transformers_embedder/embedder.py</code> <pre><code>def save_pretrained(self, save_directory: Union[str, Path]):\n    \"\"\"\n    Save a model and its configuration file to a directory.\n\n    Args:\n        save_directory (`str`, `Path`):\n            Directory to which to save.\n    \"\"\"\n    self.transformer_model.save_pretrained(save_directory)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEmbedderOutput","title":"<code>TransformersEmbedderOutput</code>  <code>dataclass</code>","text":"<p>             Bases: <code>ModelOutput</code></p> <p>Class for model's outputs.</p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>@dataclass\nclass TransformersEmbedderOutput(tr.file_utils.ModelOutput):\n    \"\"\"Class for model's outputs.\"\"\"\n\n    word_embeddings: Optional[torch.FloatTensor] = None\n    last_hidden_state: Optional[torch.FloatTensor] = None\n    pooler_output: Optional[torch.FloatTensor] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEncoder","title":"<code>TransformersEncoder</code>","text":"<p>             Bases: <code>TransformersEmbedder</code></p> <p>Transformer Embedder class.</p> <p>Word level embeddings from various transformer architectures from Huggingface Transformers API.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`, `tr.PreTrainedModel`</code> <p>Transformer model to use (https://huggingface.co/models).</p> required <code>layer_pooling_strategy</code> <code>`str`, optional, defaults to `last`</code> <p>What output to get from the transformer model. The last hidden state (<code>last</code>), the concatenation of the selected hidden layers (<code>concat</code>), the sum of the selected hidden layers (<code>sum</code>), the average of the selected hidden layers (<code>mean</code>).</p> <code>'last'</code> <code>subword_pooling_strategy</code> <code>`str`, optional, defaults to `scatter`</code> <p>What pooling strategy to use for the sub-word embeddings. Methods available are <code>scatter</code>, <code>sparse</code> and <code>none</code>. The <code>scatter</code> strategy is ONNX comptabile but uses <code>scatter_add</code> that is not deterministic. The <code>sparse</code> strategy is deterministic but it is not comptabile with ONNX.</p> <code>'sparse'</code> <code>output_layers</code> <code>`tuple`, optional, defaults to `(-4, -3, -2, -1)`</code> <p>Which hidden layers to get from the transformer model.</p> <code>(-4, -3, -2, -1)</code> <code>fine_tune</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the transformer model is fine-tuned during training.</p> <code>True</code> <code>return_all</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code>, returns all the outputs from the HuggingFace model.</p> <code>False</code> <code>projection_size</code> <code>`int`, optional, defaults to `None`</code> <p>If not <code>None</code>, the output of the transformer is projected to this size.</p> <code>None</code> <code>activation_layer</code> <code>`torch.nn.Module`, optional, defaults to `None`</code> <p>Activation layer to use. If <code>None</code>, no activation layer is used.</p> <code>None</code> <code>dropout</code> <code>`float`, optional, defaults to `0.1`</code> <p>The dropout probability.</p> <code>0.1</code> <code>bias</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the transformer model has a bias.</p> <code>True</code> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>class TransformersEncoder(TransformersEmbedder):\n    \"\"\"\n    Transformer Embedder class.\n\n    Word level embeddings from various transformer architectures from Huggingface Transformers API.\n\n    Args:\n        model (`str`, `tr.PreTrainedModel`):\n            Transformer model to use (https://huggingface.co/models).\n        layer_pooling_strategy (`str`, optional, defaults to `last`):\n            What output to get from the transformer model. The last hidden state (``last``),\n            the concatenation of the selected hidden layers (``concat``), the sum of the selected hidden\n            layers (``sum``), the average of the selected hidden layers (``mean``).\n        subword_pooling_strategy (`str`, optional, defaults to `scatter`):\n            What pooling strategy to use for the sub-word embeddings. Methods available are ``scatter``,\n            ``sparse`` and ``none``. The ``scatter`` strategy is ONNX comptabile but uses ``scatter_add``\n            that is not deterministic. The ``sparse`` strategy is deterministic but it is not comptabile\n            with ONNX.\n        output_layers (`tuple`, optional, defaults to `(-4, -3, -2, -1)`):\n            Which hidden layers to get from the transformer model.\n        fine_tune (`bool`, optional, defaults to `True`):\n            If ``True``, the transformer model is fine-tuned during training.\n        return_all (`bool`, optional, defaults to `False`):\n            If ``True``, returns all the outputs from the HuggingFace model.\n        projection_size (`int`, optional, defaults to `None`):\n            If not ``None``, the output of the transformer is projected to this size.\n        activation_layer (`torch.nn.Module`, optional, defaults to `None`):\n            Activation layer to use. If ``None``, no activation layer is used.\n        dropout (`float`, optional, defaults to `0.1`):\n            The dropout probability.\n        bias (`bool`, optional, defaults to `True`):\n            If ``True``, the transformer model has a bias.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[str, tr.PreTrainedModel],\n        layer_pooling_strategy: str = \"last\",\n        subword_pooling_strategy: str = \"sparse\",\n        output_layers: Sequence[int] = (-4, -3, -2, -1),\n        fine_tune: bool = True,\n        return_all: bool = False,\n        projection_size: Optional[int] = None,\n        activation_layer: Optional[torch.nn.Module] = None,\n        dropout: float = 0.1,\n        bias: bool = True,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        super().__init__(\n            model,\n            layer_pooling_strategy,\n            subword_pooling_strategy,\n            output_layers,\n            fine_tune,\n            return_all,\n            *args,\n            **kwargs,\n        )\n        self.encoder = Encoder(\n            self.transformer_hidden_size,\n            projection_size,\n            activation_layer,\n            dropout,\n            bias,\n        )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        scatter_offsets: Optional[torch.Tensor] = None,\n        sparse_offsets: Optional[Mapping[str, Any]] = None,\n        **kwargs,\n    ) -&gt; TransformersEmbedderOutput:\n        \"\"\"\n        Forward method of the PyTorch module.\n\n        Args:\n            input_ids (`torch.Tensor`):\n                Input ids for the transformer model.\n            attention_mask (`torch.Tensor`, optional):\n                Attention mask for the transformer model.\n            token_type_ids (`torch.Tensor`, optional):\n                Token type ids for the transformer model.\n            scatter_offsets (`torch.Tensor`, optional):\n                Offsets of the sub-word, used to reconstruct the word embeddings.\n\n        Returns:\n            `TransformersEmbedderOutput`:\n                Word level embeddings plus the output of the transformer model.\n        \"\"\"\n        transformers_kwargs = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n            \"scatter_offsets\": scatter_offsets,\n            \"sparse_offsets\": sparse_offsets,\n            **kwargs,\n        }\n        transformer_output = super().forward(**transformers_kwargs)\n        encoder_output = self.encoder(transformer_output.word_embeddings)\n        transformer_output.word_embeddings = encoder_output\n        return transformer_output\n\n    @property\n    def hidden_size(self) -&gt; int:\n        \"\"\"\n        Returns the hidden size of the transformer.\n\n        Returns:\n            `int`: Hidden size of ``self.transformer_model``.\n        \"\"\"\n        return self.encoder.projection_size\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEncoder.hidden_size","title":"<code>hidden_size: int</code>  <code>property</code>","text":"<p>Returns the hidden size of the transformer.</p> <p>Returns:</p> Type Description <code>int</code> <p><code>int</code>: Hidden size of <code>self.transformer_model</code>.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/embedder/#transformers_embedder.embedder.TransformersEncoder.forward","title":"<code>forward(input_ids, attention_mask=None, token_type_ids=None, scatter_offsets=None, sparse_offsets=None, **kwargs)</code>","text":"<p>Forward method of the PyTorch module.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>`torch.Tensor`</code> <p>Input ids for the transformer model.</p> required <code>attention_mask</code> <code>`torch.Tensor`</code> <p>Attention mask for the transformer model.</p> <code>None</code> <code>token_type_ids</code> <code>`torch.Tensor`</code> <p>Token type ids for the transformer model.</p> <code>None</code> <code>scatter_offsets</code> <code>`torch.Tensor`</code> <p>Offsets of the sub-word, used to reconstruct the word embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>TransformersEmbedderOutput</code> <p><code>TransformersEmbedderOutput</code>: Word level embeddings plus the output of the transformer model.</p> Source code in <code>transformers_embedder/embedder.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    scatter_offsets: Optional[torch.Tensor] = None,\n    sparse_offsets: Optional[Mapping[str, Any]] = None,\n    **kwargs,\n) -&gt; TransformersEmbedderOutput:\n    \"\"\"\n    Forward method of the PyTorch module.\n\n    Args:\n        input_ids (`torch.Tensor`):\n            Input ids for the transformer model.\n        attention_mask (`torch.Tensor`, optional):\n            Attention mask for the transformer model.\n        token_type_ids (`torch.Tensor`, optional):\n            Token type ids for the transformer model.\n        scatter_offsets (`torch.Tensor`, optional):\n            Offsets of the sub-word, used to reconstruct the word embeddings.\n\n    Returns:\n        `TransformersEmbedderOutput`:\n            Word level embeddings plus the output of the transformer model.\n    \"\"\"\n    transformers_kwargs = {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"token_type_ids\": token_type_ids,\n        \"scatter_offsets\": scatter_offsets,\n        \"sparse_offsets\": sparse_offsets,\n        **kwargs,\n    }\n    transformer_output = super().forward(**transformers_kwargs)\n    encoder_output = self.encoder(transformer_output.word_embeddings)\n    transformer_output.word_embeddings = encoder_output\n    return transformer_output\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/encoder/","title":"Encoder","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/encoder/#transformers_embedder.modules.encoder.Encoder","title":"<code>Encoder</code>","text":"<p>             Bases: <code>Module</code></p> <p>An encoder module for the <code>TransformersEmbedder</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>transformer_hidden_size</code> <code>`int`</code> <p>The hidden size of the inner transformer.</p> required <code>projection_size</code> <code>`int`, `optional`, defaults to `None`</code> <p>The size of the projection layer.</p> <code>None</code> <code>activation_layer</code> <code>`torch.nn.Module`, optional, defaults to `None`</code> <p>Activation layer to use. If <code>None</code>, no activation layer is used.</p> <code>None</code> <code>dropout</code> <code>`float`, `optional`, defaults to `0.1`</code> <p>The dropout value.</p> <code>0.1</code> <code>bias</code> <code>`bool`, `optional`, defaults to `True`</code> <p>Whether to use a bias.</p> <code>True</code> Source code in <code>transformers_embedder/modules/encoder.py</code> <pre><code>class Encoder(torch.nn.Module):\n    \"\"\"\n    An encoder module for the `TransformersEmbedder` class.\n\n    Args:\n        transformer_hidden_size (`int`):\n            The hidden size of the inner transformer.\n        projection_size (`int`, `optional`, defaults to `None`):\n            The size of the projection layer.\n        activation_layer (`torch.nn.Module`, optional, defaults to `None`):\n            Activation layer to use. If ``None``, no activation layer is used.\n        dropout (`float`, `optional`, defaults to `0.1`):\n            The dropout value.\n        bias (`bool`, `optional`, defaults to `True`):\n            Whether to use a bias.\n    \"\"\"\n\n    def __init__(\n        self,\n        transformer_hidden_size: int,\n        projection_size: Optional[int] = None,\n        activation_layer: Optional[torch.nn.Module] = None,\n        dropout: float = 0.1,\n        bias: bool = True,\n    ):\n        super().__init__()\n        self.projection_size = projection_size or transformer_hidden_size\n        self.projection_layer = torch.nn.Linear(\n            transformer_hidden_size, self.projection_size, bias=bias\n        )\n        self.dropout_layer = torch.nn.Dropout(dropout)\n        self.activation_layer = activation_layer\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the encoder.\n\n        Args:\n            x (`torch.Tensor`):\n                The input tensor.\n\n        Returns:\n            `torch.Tensor`: The encoded tensor.\n        \"\"\"\n        x = self.projection_layer(self.dropout_layer(x))\n        if self.activation_layer is not None:\n            x = self.activation_layer(x)\n        return x\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/encoder/#transformers_embedder.modules.encoder.Encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>`torch.Tensor`</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code>: The encoded tensor.</p> Source code in <code>transformers_embedder/modules/encoder.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the encoder.\n\n    Args:\n        x (`torch.Tensor`):\n            The input tensor.\n\n    Returns:\n        `torch.Tensor`: The encoded tensor.\n    \"\"\"\n    x = self.projection_layer(self.dropout_layer(x))\n    if self.activation_layer is not None:\n        x = self.activation_layer(x)\n    return x\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/main/","title":"Main","text":"<ul> <li>transformers_embedder<ul> <li>embedder</li> <li>modules<ul> <li>encoder</li> <li>scalar_mix</li> </ul> </li> <li>tokenizer</li> <li>utils</li> </ul> </li> </ul>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/scalar_mix/","title":"Scalar mix","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/scalar_mix/#transformers_embedder.modules.scalar_mix.ScalarMix","title":"<code>ScalarMix</code>","text":"<p>             Bases: <code>Module</code></p> <p>Computes a parameterised scalar mixture of N tensors, <code>mixture = gamma * sum(s_k * tensor_k)</code> where <code>s = softmax(w)</code>, with <code>w</code> and <code>gamma</code> scalar parameters. In addition, if <code>do_layer_norm=True</code> then apply layer normalization to each tensor before weighting.</p> Source code in <code>transformers_embedder/modules/scalar_mix.py</code> <pre><code>class ScalarMix(torch.nn.Module):\n    \"\"\"\n    Computes a parameterised scalar mixture of N tensors, `mixture = gamma * sum(s_k * tensor_k)`\n    where `s = softmax(w)`, with `w` and `gamma` scalar parameters.\n    In addition, if `do_layer_norm=True` then apply layer normalization to each tensor\n    before weighting.\n    \"\"\"\n\n    def __init__(\n        self,\n        mixture_size: int,\n        do_layer_norm: bool = False,\n        initial_scalar_parameters: List[float] = None,\n        trainable: bool = True,\n    ) -&gt; None:\n        super().__init__()\n        self.mixture_size = mixture_size\n        self.do_layer_norm = do_layer_norm\n\n        if initial_scalar_parameters is None:\n            initial_scalar_parameters = [0.0] * mixture_size\n        elif len(initial_scalar_parameters) != mixture_size:\n            raise ValueError(\n                f\"Length of `initial_scalar_parameters` {initial_scalar_parameters} differs \"\n                f\"from `mixture_size` {mixture_size}\"\n            )\n\n        self.scalar_parameters = ParameterList(\n            [\n                Parameter(\n                    torch.FloatTensor([initial_scalar_parameters[i]]),\n                    requires_grad=trainable,\n                )\n                for i in range(mixture_size)\n            ]\n        )\n        self.gamma = Parameter(torch.FloatTensor([1.0]), requires_grad=trainable)\n\n    def forward(\n        self, tensors: List[torch.Tensor], mask: torch.BoolTensor = None\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute a weighted average of the `tensors`.  The input tensors caa be any shape\n        with at least two dimensions, but must all be the same shape.\n        When `do_layer_norm=True`, the `mask` is a required input. If the `tensors` are\n        dimensioned  `(dim_0, ..., dim_{n-1}, dim_n)`, then the `mask` is dimensioned\n        `(dim_0, ..., dim_{n-1})`, as in the typical case with `tensors` of shape\n        `(batch_size, timesteps, dim)` and `mask` of shape `(batch_size, timesteps)`.\n        When `do_layer_norm=False` the `mask` is ignored.\n        \"\"\"\n        if len(tensors) != self.mixture_size:\n            raise ValueError(\n                f\"{len(tensors)} tensors were passed, but the module was initialized to \"\n                f\"mix {self.mixture_size} tensors.\"\n            )\n\n        def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):\n            tensor_masked = tensor * broadcast_mask\n            mean = torch.sum(tensor_masked) / num_elements_not_masked\n            variance = (\n                torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2)\n                / num_elements_not_masked\n            )\n            return (tensor - mean) / torch.sqrt(variance + 1e-4)\n\n        normed_weights = torch.nn.functional.softmax(\n            torch.cat([parameter for parameter in self.scalar_parameters]), dim=0\n        )\n        normed_weights = torch.split(normed_weights, split_size_or_sections=1)\n\n        if not self.do_layer_norm:\n            pieces = []\n            for weight, tensor in zip(normed_weights, tensors):\n                pieces.append(weight * tensor)\n            return self.gamma * sum(pieces)\n\n        else:\n            assert mask is not None\n            broadcast_mask = mask.unsqueeze(-1)\n            input_dim = tensors[0].size(-1)\n            num_elements_not_masked = torch.sum(mask) * input_dim\n\n            pieces = []\n            for weight, tensor in zip(normed_weights, tensors):\n                pieces.append(\n                    weight\n                    * _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked)\n                )\n            return self.gamma * sum(pieces)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/scalar_mix/#transformers_embedder.modules.scalar_mix.ScalarMix.forward","title":"<code>forward(tensors, mask=None)</code>","text":"<p>Compute a weighted average of the <code>tensors</code>.  The input tensors caa be any shape with at least two dimensions, but must all be the same shape. When <code>do_layer_norm=True</code>, the <code>mask</code> is a required input. If the <code>tensors</code> are dimensioned  <code>(dim_0, ..., dim_{n-1}, dim_n)</code>, then the <code>mask</code> is dimensioned <code>(dim_0, ..., dim_{n-1})</code>, as in the typical case with <code>tensors</code> of shape <code>(batch_size, timesteps, dim)</code> and <code>mask</code> of shape <code>(batch_size, timesteps)</code>. When <code>do_layer_norm=False</code> the <code>mask</code> is ignored.</p> Source code in <code>transformers_embedder/modules/scalar_mix.py</code> <pre><code>def forward(\n    self, tensors: List[torch.Tensor], mask: torch.BoolTensor = None\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute a weighted average of the `tensors`.  The input tensors caa be any shape\n    with at least two dimensions, but must all be the same shape.\n    When `do_layer_norm=True`, the `mask` is a required input. If the `tensors` are\n    dimensioned  `(dim_0, ..., dim_{n-1}, dim_n)`, then the `mask` is dimensioned\n    `(dim_0, ..., dim_{n-1})`, as in the typical case with `tensors` of shape\n    `(batch_size, timesteps, dim)` and `mask` of shape `(batch_size, timesteps)`.\n    When `do_layer_norm=False` the `mask` is ignored.\n    \"\"\"\n    if len(tensors) != self.mixture_size:\n        raise ValueError(\n            f\"{len(tensors)} tensors were passed, but the module was initialized to \"\n            f\"mix {self.mixture_size} tensors.\"\n        )\n\n    def _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked):\n        tensor_masked = tensor * broadcast_mask\n        mean = torch.sum(tensor_masked) / num_elements_not_masked\n        variance = (\n            torch.sum(((tensor_masked - mean) * broadcast_mask) ** 2)\n            / num_elements_not_masked\n        )\n        return (tensor - mean) / torch.sqrt(variance + 1e-4)\n\n    normed_weights = torch.nn.functional.softmax(\n        torch.cat([parameter for parameter in self.scalar_parameters]), dim=0\n    )\n    normed_weights = torch.split(normed_weights, split_size_or_sections=1)\n\n    if not self.do_layer_norm:\n        pieces = []\n        for weight, tensor in zip(normed_weights, tensors):\n            pieces.append(weight * tensor)\n        return self.gamma * sum(pieces)\n\n    else:\n        assert mask is not None\n        broadcast_mask = mask.unsqueeze(-1)\n        input_dim = tensors[0].size(-1)\n        num_elements_not_masked = torch.sum(mask) * input_dim\n\n        pieces = []\n        for weight, tensor in zip(normed_weights, tensors):\n            pieces.append(\n                weight\n                * _do_layer_norm(tensor, broadcast_mask, num_elements_not_masked)\n            )\n        return self.gamma * sum(pieces)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/","title":"Tokenizer","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.ModelInputs","title":"<code>ModelInputs</code>","text":"<p>             Bases: <code>UserDict</code></p> <p>Model input dictionary wrapper.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>class ModelInputs(UserDict):\n    \"\"\"Model input dictionary wrapper.\"\"\"\n\n    def __getattr__(self, item: str):\n        try:\n            return self.data[item]\n        except KeyError:\n            raise AttributeError(f\"`ModelInputs` has no attribute `{item}`\")\n\n    def __getitem__(self, item: str) -&gt; Any:\n        return self.data[item]\n\n    def __getstate__(self):\n        return {\"data\": self.data}\n\n    def __setstate__(self, state):\n        if \"data\" in state:\n            self.data = state[\"data\"]\n\n    def keys(self):\n        \"\"\"A set-like object providing a view on D's keys.\"\"\"\n        return self.data.keys()\n\n    def values(self):\n        \"\"\"An object providing a view on D's values.\"\"\"\n        return self.data.values()\n\n    def items(self):\n        \"\"\"A set-like object providing a view on D's items.\"\"\"\n        return self.data.items()\n\n    def to(self, device: Union[str, torch.device]) -&gt; ModelInputs:\n        \"\"\"\n        Send all tensors values to device.\n\n        Args:\n            device (`str` or `torch.device`): The device to put the tensors on.\n\n        Returns:\n            :class:`tokenizers.ModelInputs`: The same instance of :class:`~tokenizers.ModelInputs`\n            after modification.\n        \"\"\"\n        if isinstance(device, (str, torch.device, int)):\n            self.data = {\n                k: v.to(device=device) if hasattr(v, \"to\") else v\n                for k, v in self.data.items()\n            }\n        else:\n            logger.warning(\n                f\"Attempting to cast to another type, {str(device)}. This is not supported.\"\n            )\n        return self\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.ModelInputs.items","title":"<code>items()</code>","text":"<p>A set-like object providing a view on D's items.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def items(self):\n    \"\"\"A set-like object providing a view on D's items.\"\"\"\n    return self.data.items()\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.ModelInputs.keys","title":"<code>keys()</code>","text":"<p>A set-like object providing a view on D's keys.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def keys(self):\n    \"\"\"A set-like object providing a view on D's keys.\"\"\"\n    return self.data.keys()\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.ModelInputs.to","title":"<code>to(device)</code>","text":"<p>Send all tensors values to device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>`str` or `torch.device`</code> <p>The device to put the tensors on.</p> required <p>Returns:</p> Type Description <code>ModelInputs</code> <p>class:<code>tokenizers.ModelInputs</code>: The same instance of :class:<code>~tokenizers.ModelInputs</code></p> <code>ModelInputs</code> <p>after modification.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def to(self, device: Union[str, torch.device]) -&gt; ModelInputs:\n    \"\"\"\n    Send all tensors values to device.\n\n    Args:\n        device (`str` or `torch.device`): The device to put the tensors on.\n\n    Returns:\n        :class:`tokenizers.ModelInputs`: The same instance of :class:`~tokenizers.ModelInputs`\n        after modification.\n    \"\"\"\n    if isinstance(device, (str, torch.device, int)):\n        self.data = {\n            k: v.to(device=device) if hasattr(v, \"to\") else v\n            for k, v in self.data.items()\n        }\n    else:\n        logger.warning(\n            f\"Attempting to cast to another type, {str(device)}. This is not supported.\"\n        )\n    return self\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.ModelInputs.values","title":"<code>values()</code>","text":"<p>An object providing a view on D's values.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def values(self):\n    \"\"\"An object providing a view on D's values.\"\"\"\n    return self.data.values()\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer","title":"<code>Tokenizer</code>","text":"<p>A wrapper class for HuggingFace Tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`, `transformers.PreTrainedTokenizer`</code> <p>Language model name (or a transformer <code>PreTrainedTokenizer</code>.</p> required <code>return_sparse_offsets</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the sparse offsets of the tokens in the input text are returned. To reduce memory usage, set this to <code>False</code> if you don't need them, e.g. you set the <code>subword_pooling_strategy</code> to <code>scatter</code> in the <code>TransformersEmbedder</code> model.</p> <code>True</code> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>class Tokenizer:\n    \"\"\"\n    A wrapper class for HuggingFace Tokenizer.\n\n    Args:\n        model (`str`, `transformers.PreTrainedTokenizer`):\n            Language model name (or a transformer `PreTrainedTokenizer`.\n        return_sparse_offsets (`bool`, optional, defaults to `True`):\n            If `True`, the sparse offsets of the tokens in the input text are returned. To reduce\n            memory usage, set this to `False` if you don't need them, e.g. you set the\n            `subword_pooling_strategy` to `scatter` in the `TransformersEmbedder` model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[str, tr.PreTrainedTokenizer],\n        return_sparse_offsets: bool = True,\n        *args,\n        **kwargs,\n    ):\n        if isinstance(model, str):\n            # init HuggingFace tokenizer\n            self.huggingface_tokenizer = tr.AutoTokenizer.from_pretrained(\n                model, *args, **kwargs\n            )\n            # get config\n            self.config = tr.AutoConfig.from_pretrained(model, *args, **kwargs)\n        else:\n            self.huggingface_tokenizer = model\n            self.config = tr.AutoConfig.from_pretrained(\n                self.huggingface_tokenizer.name_or_path, *args, **kwargs\n            )\n\n        self.return_sparse_offsets = return_sparse_offsets\n\n        # padding stuff\n        # default, batch length is model max length\n        self.subword_max_batch_len = self.huggingface_tokenizer.model_max_length\n        self.word_max_batch_len = self.huggingface_tokenizer.model_max_length\n        # padding ops\n        self.padding_ops = {}\n        # keys that will be converted in tensors\n        self.to_tensor_inputs = set()\n\n    def __len__(self):\n        \"\"\"Size of the full vocabulary with the added tokens.\"\"\"\n        return len(self.huggingface_tokenizer)\n\n    def __call__(\n        self,\n        text: Union[str, List[str], List[List[str]]],\n        text_pair: Union[str, List[str], List[List[str]], None] = None,\n        padding: Union[bool, str, PaddingStrategy] = False,\n        truncation: Union[bool, str, TruncationStrategy] = False,\n        max_length: Optional[int] = None,\n        return_tensors: Optional[Union[bool, str]] = None,\n        is_split_into_words: bool = False,\n        additional_inputs: Optional[Dict[str, Any]] = None,\n        *args,\n        **kwargs,\n    ) -&gt; ModelInputs:\n        \"\"\"\n        Prepare the text in input for models that uses HuggingFace as embeddings.\n\n        Args:\n            text (`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`):\n                Text or batch of text to be encoded.\n            text_pair (`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`):\n                Text or batch of text to be encoded.\n            padding (`bool`, optional, defaults to `False`):\n                If `True`, applies padding to the batch based on the maximum length of the batch.\n            max_length (`int`, optional, defaults to `None`):\n                If specified, truncates the input sequence to that value. Otherwise,\n                uses the model max length.\n            return_tensors (`bool`, optional, defaults to `None`):\n                If `True`, the outputs is converted to `torch.Tensor`\n            is_split_into_words (`bool`, optional, defaults to `False`):\n                If `True` and the input is a string, the input is split on spaces.\n            additional_inputs (`Dict[str, Any]`, optional, defaults to `None`):\n                Additional inputs to be passed to the model.\n\n        Returns:\n            `ModelInputs`: The inputs to the transformer model.\n        \"\"\"\n        # some checks before starting\n        if return_tensors == \"tf\":\n            raise ValueError(\n                \"`return_tensors='tf'` is not supported. Please use `return_tensors='pt'` \"\n                \"or `return_tensors=True`.\"\n            )\n        if return_tensors is True:\n            return_tensors = \"pt\"\n        if return_tensors is False:\n            return_tensors = None\n\n        # check if input is batched or a single sample\n        is_batched = bool(\n            isinstance(text, (list, tuple))\n            and text\n            and (\n                (isinstance(text[0], (list, tuple)) and is_split_into_words)\n                or isinstance(text[0], str)\n            )\n        )\n        if not is_batched:  # batch it\n            text = [text]\n            text_pair = [text_pair] if text_pair is not None else None\n\n        # use huggingface tokenizer to encode the text\n        model_inputs = self.huggingface_tokenizer(\n            text,\n            text_pair=text_pair,\n            padding=padding,\n            truncation=truncation,\n            max_length=max_length,\n            is_split_into_words=is_split_into_words,\n            return_tensors=return_tensors,\n            *args,\n            **kwargs,\n        )\n        # build the offsets used to pool the subwords\n        scatter_offsets, sentence_lengths = self.build_scatter_offsets(\n            model_inputs,\n            return_tensors=return_tensors,\n            there_is_text_pair=text_pair is not None,\n        )\n\n        # convert to ModelInputs\n        model_inputs = ModelInputs(**model_inputs)\n        # add the offsets to the model inputs\n        model_inputs.update(\n            {\"scatter_offsets\": scatter_offsets, \"sentence_lengths\": sentence_lengths}\n        )\n\n        if self.return_sparse_offsets:\n            # build the data used to pool the subwords when in sparse mode\n            bpe_info: Mapping[str, Any] = self.build_sparse_offsets(\n                offsets=scatter_offsets,\n                bpe_mask=model_inputs.attention_mask,\n                words_per_sentence=sentence_lengths,\n            )\n            # add the bpe info to the model inputs\n            model_inputs[\"sparse_offsets\"] = ModelInputs(**bpe_info)\n\n        # we also update the maximum batch length,\n        # both for subword and word level\n        self.subword_max_batch_len = max(len(x) for x in model_inputs.input_ids)\n        self.word_max_batch_len = max(x for x in model_inputs.sentence_lengths)\n\n        # check if we need to convert other stuff to tensors\n        if additional_inputs:\n            model_inputs.update(additional_inputs)\n            # check if there is a padding strategy\n            if padding:\n                missing_keys = set(additional_inputs.keys()) - set(\n                    self.padding_ops.keys()\n                )\n                if missing_keys:\n                    raise ValueError(\n                        f\"There are no padding strategies for the following keys: {missing_keys}. \"\n                        \"Please add one with `tokenizer.add_padding_ops()`.\"\n                    )\n                self.pad_batch(model_inputs)\n            # convert them to tensors\n            if return_tensors == \"pt\":\n                self.to_tensor(model_inputs)\n\n        return model_inputs\n\n    def build_scatter_offsets(\n        self,\n        model_inputs: BatchEncoding,\n        return_tensors: bool = True,\n        there_is_text_pair: bool = False,\n    ) -&gt; Tuple:\n        \"\"\"\n        Build the offset tensor for the batch of inputs.\n\n        Args:\n            model_inputs (`BatchEncoding`):\n                The inputs to the transformer model.\n            return_tensors (`bool`, optional, defaults to `True`):\n                If `True`, the outputs is converted to `torch.Tensor`\n            there_is_text_pair (`bool`, optional, defaults to `False`):\n                If `True` `text_pair` is not None.\n\n        Returns:\n            `List[List[int]]` or `torch.Tensor`: The offsets of the sub-tokens.\n        \"\"\"\n        # output data structure\n        offsets = []\n        sentence_lengths = []\n        # model_inputs should be the output of the HuggingFace tokenizer\n        # it contains the word offsets to reconstruct the original tokens from the\n        # sub-tokens\n        for batch_index in range(len(model_inputs.input_ids)):\n            word_ids = model_inputs.word_ids(batch_index)\n            # it is slightly different from what we need, so here we make it compatible\n            # with our subword pooling strategy\n            # if the first token is a special token, we need to take it into account\n            if self.has_starting_token:\n                word_offsets = [0] + [\n                    w + 1 if w is not None else w for w in word_ids[1:]\n                ]\n            # otherwise, we can just use word_ids as is\n            else:\n                word_offsets = word_ids\n\n            # replace first None occurrence with sep_offset\n            sep_index = word_offsets.index(None)\n\n            # here we retrieve the max offset for the sample, which will be used as SEP offset\n            # and also as padding value for the offsets\n            sep_offset_value = max([w for w in word_offsets[:sep_index] if w is not None]) + 1\n\n            word_offsets[sep_index] = sep_offset_value\n            # if there is a text pair, we need to adjust the offsets for the second text\n            if there_is_text_pair:\n                # some models have two SEP tokens in between the two texts\n                if self.has_double_sep:\n                    sep_index += 1\n                    sep_offset_value += 1\n                    word_offsets[sep_index] = sep_offset_value\n                # keep the first offsets as is, adjust the second ones\n                word_offsets = word_offsets[: sep_index + 1] + [\n                    w + sep_offset_value if w is not None else w\n                    for w in word_offsets[sep_index + 1 :]\n                ]\n                # update again the sep_offset\n                sep_offset_value = max([w for w in word_offsets if w is not None]) + 1\n                # replace first None occurrence with sep_offset, it should be the last SEP\n                sep_index = word_offsets.index(None)\n                word_offsets[sep_index] = sep_offset_value\n            # keep track of the maximum offset for padding\n            offsets.append(word_offsets)\n            sentence_lengths.append(sep_offset_value + 1)\n        # replace remaining None occurrences with -1\n        # the remaining None occurrences are the padding values\n        offsets = [[o if o is not None else -1 for o in offset] for offset in offsets]\n        # if return_tensor is True, we need to convert the offsets to tensors\n        if return_tensors:\n            offsets = torch.as_tensor(offsets)\n        return offsets, sentence_lengths\n\n    @staticmethod\n    def build_sparse_offsets(\n        offsets: torch.Tensor | Sequence[Sequence[int]],\n        bpe_mask: torch.Tensor | Sequence[Sequence[int]],\n        words_per_sentence: Sequence[int],\n    ) -&gt; Mapping[str, Any]:\n        \"\"\"Build tensors used as info for BPE pooling, starting from the BPE offsets.\n\n        Args:\n            offsets (`torch.Tensor` or `List[List[int]]`):\n                The offsets to compute lengths from.\n            bpe_mask (`torch.Tensor` or `List[List[int]]`):\n                The attention mask at BPE level.\n            words_per_sentence (`List[int]`):\n                The sentence lengths, word-wise.\n\n        Returns:\n            `Mapping[str, Any]`: Tensors used to construct the sparse one which pools the\n            transformer encoding word-wise.\n        \"\"\"\n        if not isinstance(offsets, torch.Tensor):\n            offsets: torch.Tensor = torch.as_tensor(offsets)\n        if not isinstance(bpe_mask, torch.Tensor):\n            bpe_mask: torch.Tensor = torch.as_tensor(bpe_mask)\n\n        sentence_lengths: torch.Tensor = bpe_mask.sum(dim=1)\n\n        # We want to build triplets as coordinates (document, word, bpe)\n        # We start by creating the document index for each triplet\n        document_indices = torch.arange(offsets.size(0)).repeat_interleave(\n            sentence_lengths\n        )\n        # then the word indices\n        word_indices = offsets[offsets != -1]\n        # lastly the bpe indices\n        max_range: torch.Tensor = torch.arange(bpe_mask.shape[1])\n        bpe_indices: torch.LongTensor = torch.cat(\n            [max_range[:i] for i in bpe_mask.sum(dim=1)], dim=0\n        ).long()\n\n        unique_words, word_lengths = torch.unique_consecutive(\n            offsets, return_counts=True\n        )\n        unpadded_word_lengths = word_lengths[unique_words != -1]\n\n        # and their weight to be used as multiplication factors\n        bpe_weights: torch.FloatTensor = (\n            (1 / unpadded_word_lengths).repeat_interleave(unpadded_word_lengths).float()\n        )\n\n        sparse_indices = torch.stack(\n            [document_indices, word_indices, bpe_indices], dim=0\n        )\n\n        bpe_shape = torch.Size(\n            (\n                bpe_mask.size(0),  # batch_size\n                max(words_per_sentence),  # max number of words per sentence\n                bpe_mask.size(1),  # max bpe_number in batch wrt the sentence\n            )\n        )\n\n        return dict(\n            sparse_indices=sparse_indices,\n            sparse_values=bpe_weights,\n            sparse_size=bpe_shape,\n        )\n\n    def pad_batch(\n        self,\n        batch: Union[ModelInputs, Dict[str, list]],\n        max_length: Optional[int] = None,\n    ) -&gt; ModelInputs:\n        \"\"\"\n        Pad the batch to its maximum length or to the specified `max_length`.\n\n        Args:\n            batch (`Dict[str, list]`):\n                The batch to pad.\n            max_length (`int`, optional):\n                Override maximum length of the batch.\n\n        Returns:\n            `Dict[str, list]`: The padded batch.\n        \"\"\"\n        if max_length:\n            self.subword_max_batch_len = max_length\n            self.word_max_batch_len = max_length\n        else:\n            # get maximum len inside a batch\n            self.subword_max_batch_len = max(len(x) for x in batch[\"input_ids\"])\n            self.word_max_batch_len = max(x for x in batch[\"sentence_lengths\"])\n\n        for key in batch:\n            if key in self.padding_ops:\n                batch[key] = [self.padding_ops[key](b) for b in batch[key]]\n\n        return ModelInputs(batch)\n\n    def pad_sequence(\n        self,\n        sequence: Union[List, torch.Tensor],\n        value: int,\n        length: Union[int, str] = \"subword\",\n        pad_to_left: bool = False,\n    ) -&gt; Union[List, torch.Tensor]:\n        \"\"\"\n        Pad the input to the specified length with the given value.\n\n        Args:\n            sequence (`List`, `torch.Tensor`):\n                Element to pad, it can be either a `List` or a `torch.Tensor`.\n            value (`int`):\n                Value to use as padding.\n            length (`int`, `str`, optional, defaults to `subword`):\n                Length after pad.\n            pad_to_left (`bool`, optional, defaults to `False`):\n                If `True`, pads to the left, right otherwise.\n\n        Returns:\n            `List`, `torch.Tensor`: The padded sequence.\n        \"\"\"\n        if length == \"subword\":\n            length = self.subword_max_batch_len\n        elif length == \"word\":\n            length = self.word_max_batch_len\n        else:\n            if not isinstance(length, int):\n                raise ValueError(\n                    f\"`length` must be an `int`, `subword` or `word`. Current value is `{length}`\"\n                )\n        padding = [value] * abs(length - len(sequence))\n        if isinstance(sequence, torch.Tensor):\n            if len(sequence.shape) &gt; 1:\n                raise ValueError(\n                    f\"Sequence tensor must be 1D. Current shape is `{len(sequence.shape)}`\"\n                )\n            padding = torch.as_tensor(padding)\n        if pad_to_left:\n            if isinstance(sequence, torch.Tensor):\n                return torch.cat((padding, sequence), -1)\n            return padding + sequence\n        if isinstance(sequence, torch.Tensor):\n            return torch.cat((sequence, padding), -1)\n        return sequence + padding\n\n    def add_special_tokens(\n        self, special_tokens_dict: Dict[str, Union[str, tr.AddedToken]]\n    ) -&gt; int:\n        \"\"\"\n        Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder.\n        If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last\n        index of the current vocabulary).\n\n        Args:\n            special_tokens_dict (`Dict`):\n                The dictionary containing special tokens. Keys should be in\n                the list of predefined special attributes: [``bos_token``, ``eos_token``,\n                ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n        Returns:\n            `int`: Number of tokens added to the vocabulary.\n        \"\"\"\n        return self.huggingface_tokenizer.add_special_tokens(special_tokens_dict)\n\n    def add_padding_ops(self, key: str, value: Any, length: Union[int, str]):\n        \"\"\"\n        Add padding logic to custom fields.\n        If the field is not in `self.to_tensor_inputs`, this method will add the key to it.\n\n        Args:\n            key (`str`):\n                Name of the field in the tokenizer input.\n            value (`Any`):\n                Value to use for padding.\n            length (`int`, `str`):\n                Length to pad. It can be an `int`, or two string value\n                - ``subword``: the element is padded to the batch max length relative to the subwords length\n                - ``word``: the element is padded to the batch max length relative to the original word length\n        \"\"\"\n        if key not in self.to_tensor_inputs:\n            self.to_tensor_inputs.add(key)\n        self.padding_ops[key] = partial(self.pad_sequence, value=value, length=length)\n\n    def add_to_tensor_inputs(self, names: Union[str, Sequence[str]]) -&gt; Set[str]:\n        \"\"\"\n        Add these keys to the ones that will be converted in Tensors.\n\n        Args:\n            names (`str`, `set`):\n                Name of the field (or fields) to convert to tensors.\n\n        Returns:\n            `set`: The set of keys that will be converted to tensors.\n        \"\"\"\n        if isinstance(names, str):\n            names = {names}\n        if not isinstance(names, set):\n            names = set(names)\n        self.to_tensor_inputs |= names\n        return self.to_tensor_inputs\n\n    def to_tensor(self, batch: Union[ModelInputs, List[dict], dict]) -&gt; ModelInputs:\n        \"\"\"\n        Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in\n        `self.to_tensor_inputs`. By default, only the standard model inputs are converted. Use\n        `self.add_to_tensor_inputs` to add custom fields.\n\n        Args:\n            batch (`List[dict]`, `dict`):\n                Batch in input.\n\n        Returns:\n            `ModelInputs`: The batch as tensor.\n        \"\"\"\n        # convert to tensor\n        batch = {\n            k: torch.as_tensor(v)\n            if k in self.to_tensor_inputs and not isinstance(v, torch.Tensor)\n            else v\n            for k, v in batch.items()\n        }\n        return ModelInputs(batch)\n\n    @staticmethod\n    def _clean_output(output: Union[List, Dict]) -&gt; Dict:\n        \"\"\"\n        Clean before output.\n\n        Args:\n            output (:obj`List[dict]`, `dict`):\n                The output to clean.\n\n        Returns:\n            `dict`: The cleaned output.\n        \"\"\"\n        # single sentence case, generalize\n        if isinstance(output, dict):\n            output = [output]\n        # convert list to dict\n        output = {k: [d[k] for d in output] for k in output[0]}\n        return output\n\n    @staticmethod\n    def _get_token_type_id(config: tr.PretrainedConfig) -&gt; int:\n        \"\"\"\n        Get token type id. Useful when dealing with models that don't accept 1 as type id.\n        Args:\n            config (`transformers.PretrainedConfig`):\n                Transformer config.\n\n        Returns:\n            `int`: Correct token type id for that model.\n        \"\"\"\n        if hasattr(config, \"type_vocab_size\"):\n            return 1 if config.type_vocab_size == 2 else 0\n        return 0\n\n    @staticmethod\n    def _type_checking(text: Any, text_pair: Any):\n        \"\"\"\n        Checks type of the inputs.\n\n        Args:\n            text (`Any`):\n                Text to check.\n            text_pair (`Any`):\n                Text pair to check.\n\n        Returns:\n        \"\"\"\n\n        def is_type_correct(text_to_check: Any) -&gt; bool:\n            \"\"\"\n            Check if input type is correct, returning a boolean value.\n\n            Args:\n                text_to_check (`Any`):\n                    text to check.\n\n            Returns:\n                :obj`bool`: :obj`True` if the type is correct.\n            \"\"\"\n            return (\n                text_to_check is None\n                or isinstance(text_to_check, str)\n                or (\n                    isinstance(text_to_check, (list, tuple))\n                    and (\n                        len(text_to_check) == 0\n                        or (\n                            isinstance(text_to_check[0], str)\n                            or (\n                                isinstance(text_to_check[0], (list, tuple))\n                                and (\n                                    len(text_to_check[0]) == 0\n                                    or isinstance(text_to_check[0][0], str)\n                                )\n                            )\n                        )\n                    )\n                )\n            )\n\n        if not is_type_correct(text):\n            raise AssertionError(\n                \"text input must of type `str` (single example), `List[str]` (batch or single \"\n                \"pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).\"\n            )\n\n        if not is_type_correct(text_pair):\n            raise AssertionError(\n                \"text_pair input must be `str` (single example), `List[str]` (batch or single \"\n                \"pre-tokenized example) or `List[List[str]]` (batch of pre-tokenized examples).\"\n            )\n\n    @property\n    def num_special_tokens(self) -&gt; int:\n        \"\"\"\n        Return the number of special tokens the model needs.\n        It assumes the input contains both sentences (`text` and `text_pair`).\n\n        Returns:\n            `int`: the number of special tokens.\n        \"\"\"\n        if isinstance(\n            self.huggingface_tokenizer, MODELS_WITH_DOUBLE_SEP\n        ) and isinstance(self.huggingface_tokenizer, MODELS_WITH_STARTING_TOKEN):\n            return 4\n        if isinstance(\n            self.huggingface_tokenizer,\n            (MODELS_WITH_DOUBLE_SEP, MODELS_WITH_STARTING_TOKEN),\n        ):\n            return 3\n        return 2\n\n    @property\n    def has_double_sep(self):\n        \"\"\"True if tokenizer uses two SEP tokens.\"\"\"\n        return isinstance(self.huggingface_tokenizer, MODELS_WITH_DOUBLE_SEP)\n\n    @property\n    def has_starting_token(self):\n        \"\"\"True if tokenizer uses a starting token.\"\"\"\n        return isinstance(self.huggingface_tokenizer, MODELS_WITH_STARTING_TOKEN)\n\n    @property\n    def token_type_id(self):\n        \"\"\"Padding token.\"\"\"\n        return self._get_token_type_id(self.config)\n\n    @property\n    def pad_token(self):\n        \"\"\"Padding token.\"\"\"\n        return self.huggingface_tokenizer.pad_token\n\n    @property\n    def pad_token_id(self):\n        \"\"\"Padding token id.\"\"\"\n        return self.huggingface_tokenizer.pad_token_id\n\n    @property\n    def unk_token(self):\n        \"\"\"Unknown token.\"\"\"\n        return self.huggingface_tokenizer.unk_token\n\n    @property\n    def unk_token_id(self):\n        \"\"\"Unknown token id.\"\"\"\n        return self.huggingface_tokenizer.unk_token_id\n\n    @property\n    def cls_token(self):\n        \"\"\"\n        Classification token.\n        To extract a summary of an input sequence leveraging self-attention along the\n        full depth of the model.\n        \"\"\"\n        return self.huggingface_tokenizer.cls_token\n\n    @property\n    def cls_token_id(self):\n        \"\"\"\n        Classification token id.\n        To extract a summary of an input sequence leveraging self-attention along the\n        full depth of the model.\n        \"\"\"\n        return self.huggingface_tokenizer.cls_token_id\n\n    @property\n    def sep_token(self):\n        \"\"\"Separation token, to separate context and query in an input sequence.\"\"\"\n        return self.huggingface_tokenizer.sep_token\n\n    @property\n    def sep_token_id(self):\n        \"\"\"Separation token id, to separate context and query in an input sequence.\"\"\"\n        return self.huggingface_tokenizer.sep_token_id\n\n    @property\n    def bos_token(self):\n        \"\"\"Beginning of sentence token.\"\"\"\n        return self.huggingface_tokenizer.bos_token\n\n    @property\n    def bos_token_id(self):\n        \"\"\"Beginning of sentence token id.\"\"\"\n        return self.huggingface_tokenizer.bos_token_id\n\n    @property\n    def eos_token(self):\n        \"\"\"End of sentence token.\"\"\"\n        return self.huggingface_tokenizer.eos_token\n\n    @property\n    def eos_token_id(self):\n        \"\"\"End of sentence token id.\"\"\"\n        return self.huggingface_tokenizer.eos_token_id\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.bos_token","title":"<code>bos_token</code>  <code>property</code>","text":"<p>Beginning of sentence token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.bos_token_id","title":"<code>bos_token_id</code>  <code>property</code>","text":"<p>Beginning of sentence token id.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.cls_token","title":"<code>cls_token</code>  <code>property</code>","text":"<p>Classification token. To extract a summary of an input sequence leveraging self-attention along the full depth of the model.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.cls_token_id","title":"<code>cls_token_id</code>  <code>property</code>","text":"<p>Classification token id. To extract a summary of an input sequence leveraging self-attention along the full depth of the model.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.eos_token","title":"<code>eos_token</code>  <code>property</code>","text":"<p>End of sentence token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.eos_token_id","title":"<code>eos_token_id</code>  <code>property</code>","text":"<p>End of sentence token id.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.has_double_sep","title":"<code>has_double_sep</code>  <code>property</code>","text":"<p>True if tokenizer uses two SEP tokens.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.has_starting_token","title":"<code>has_starting_token</code>  <code>property</code>","text":"<p>True if tokenizer uses a starting token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.num_special_tokens","title":"<code>num_special_tokens: int</code>  <code>property</code>","text":"<p>Return the number of special tokens the model needs. It assumes the input contains both sentences (<code>text</code> and <code>text_pair</code>).</p> <p>Returns:</p> Type Description <code>int</code> <p><code>int</code>: the number of special tokens.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.pad_token","title":"<code>pad_token</code>  <code>property</code>","text":"<p>Padding token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.pad_token_id","title":"<code>pad_token_id</code>  <code>property</code>","text":"<p>Padding token id.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.sep_token","title":"<code>sep_token</code>  <code>property</code>","text":"<p>Separation token, to separate context and query in an input sequence.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.sep_token_id","title":"<code>sep_token_id</code>  <code>property</code>","text":"<p>Separation token id, to separate context and query in an input sequence.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.token_type_id","title":"<code>token_type_id</code>  <code>property</code>","text":"<p>Padding token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.unk_token","title":"<code>unk_token</code>  <code>property</code>","text":"<p>Unknown token.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.unk_token_id","title":"<code>unk_token_id</code>  <code>property</code>","text":"<p>Unknown token id.</p>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.__call__","title":"<code>__call__(text, text_pair=None, padding=False, truncation=False, max_length=None, return_tensors=None, is_split_into_words=False, additional_inputs=None, *args, **kwargs)</code>","text":"<p>Prepare the text in input for models that uses HuggingFace as embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`</code> <p>Text or batch of text to be encoded.</p> required <code>text_pair</code> <code>`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`</code> <p>Text or batch of text to be encoded.</p> <code>None</code> <code>padding</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code>, applies padding to the batch based on the maximum length of the batch.</p> <code>False</code> <code>max_length</code> <code>`int`, optional, defaults to `None`</code> <p>If specified, truncates the input sequence to that value. Otherwise, uses the model max length.</p> <code>None</code> <code>return_tensors</code> <code>`bool`, optional, defaults to `None`</code> <p>If <code>True</code>, the outputs is converted to <code>torch.Tensor</code></p> <code>None</code> <code>is_split_into_words</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code> and the input is a string, the input is split on spaces.</p> <code>False</code> <code>additional_inputs</code> <code>`Dict[str, Any]`, optional, defaults to `None`</code> <p>Additional inputs to be passed to the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModelInputs</code> <p><code>ModelInputs</code>: The inputs to the transformer model.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def __call__(\n    self,\n    text: Union[str, List[str], List[List[str]]],\n    text_pair: Union[str, List[str], List[List[str]], None] = None,\n    padding: Union[bool, str, PaddingStrategy] = False,\n    truncation: Union[bool, str, TruncationStrategy] = False,\n    max_length: Optional[int] = None,\n    return_tensors: Optional[Union[bool, str]] = None,\n    is_split_into_words: bool = False,\n    additional_inputs: Optional[Dict[str, Any]] = None,\n    *args,\n    **kwargs,\n) -&gt; ModelInputs:\n    \"\"\"\n    Prepare the text in input for models that uses HuggingFace as embeddings.\n\n    Args:\n        text (`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`):\n            Text or batch of text to be encoded.\n        text_pair (`str`, `List[str]`, `List[List[str]]`, `List[List[Word]]`, `List[Word]`):\n            Text or batch of text to be encoded.\n        padding (`bool`, optional, defaults to `False`):\n            If `True`, applies padding to the batch based on the maximum length of the batch.\n        max_length (`int`, optional, defaults to `None`):\n            If specified, truncates the input sequence to that value. Otherwise,\n            uses the model max length.\n        return_tensors (`bool`, optional, defaults to `None`):\n            If `True`, the outputs is converted to `torch.Tensor`\n        is_split_into_words (`bool`, optional, defaults to `False`):\n            If `True` and the input is a string, the input is split on spaces.\n        additional_inputs (`Dict[str, Any]`, optional, defaults to `None`):\n            Additional inputs to be passed to the model.\n\n    Returns:\n        `ModelInputs`: The inputs to the transformer model.\n    \"\"\"\n    # some checks before starting\n    if return_tensors == \"tf\":\n        raise ValueError(\n            \"`return_tensors='tf'` is not supported. Please use `return_tensors='pt'` \"\n            \"or `return_tensors=True`.\"\n        )\n    if return_tensors is True:\n        return_tensors = \"pt\"\n    if return_tensors is False:\n        return_tensors = None\n\n    # check if input is batched or a single sample\n    is_batched = bool(\n        isinstance(text, (list, tuple))\n        and text\n        and (\n            (isinstance(text[0], (list, tuple)) and is_split_into_words)\n            or isinstance(text[0], str)\n        )\n    )\n    if not is_batched:  # batch it\n        text = [text]\n        text_pair = [text_pair] if text_pair is not None else None\n\n    # use huggingface tokenizer to encode the text\n    model_inputs = self.huggingface_tokenizer(\n        text,\n        text_pair=text_pair,\n        padding=padding,\n        truncation=truncation,\n        max_length=max_length,\n        is_split_into_words=is_split_into_words,\n        return_tensors=return_tensors,\n        *args,\n        **kwargs,\n    )\n    # build the offsets used to pool the subwords\n    scatter_offsets, sentence_lengths = self.build_scatter_offsets(\n        model_inputs,\n        return_tensors=return_tensors,\n        there_is_text_pair=text_pair is not None,\n    )\n\n    # convert to ModelInputs\n    model_inputs = ModelInputs(**model_inputs)\n    # add the offsets to the model inputs\n    model_inputs.update(\n        {\"scatter_offsets\": scatter_offsets, \"sentence_lengths\": sentence_lengths}\n    )\n\n    if self.return_sparse_offsets:\n        # build the data used to pool the subwords when in sparse mode\n        bpe_info: Mapping[str, Any] = self.build_sparse_offsets(\n            offsets=scatter_offsets,\n            bpe_mask=model_inputs.attention_mask,\n            words_per_sentence=sentence_lengths,\n        )\n        # add the bpe info to the model inputs\n        model_inputs[\"sparse_offsets\"] = ModelInputs(**bpe_info)\n\n    # we also update the maximum batch length,\n    # both for subword and word level\n    self.subword_max_batch_len = max(len(x) for x in model_inputs.input_ids)\n    self.word_max_batch_len = max(x for x in model_inputs.sentence_lengths)\n\n    # check if we need to convert other stuff to tensors\n    if additional_inputs:\n        model_inputs.update(additional_inputs)\n        # check if there is a padding strategy\n        if padding:\n            missing_keys = set(additional_inputs.keys()) - set(\n                self.padding_ops.keys()\n            )\n            if missing_keys:\n                raise ValueError(\n                    f\"There are no padding strategies for the following keys: {missing_keys}. \"\n                    \"Please add one with `tokenizer.add_padding_ops()`.\"\n                )\n            self.pad_batch(model_inputs)\n        # convert them to tensors\n        if return_tensors == \"pt\":\n            self.to_tensor(model_inputs)\n\n    return model_inputs\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.__len__","title":"<code>__len__()</code>","text":"<p>Size of the full vocabulary with the added tokens.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def __len__(self):\n    \"\"\"Size of the full vocabulary with the added tokens.\"\"\"\n    return len(self.huggingface_tokenizer)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.add_padding_ops","title":"<code>add_padding_ops(key, value, length)</code>","text":"<p>Add padding logic to custom fields. If the field is not in <code>self.to_tensor_inputs</code>, this method will add the key to it.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>`str`</code> <p>Name of the field in the tokenizer input.</p> required <code>value</code> <code>`Any`</code> <p>Value to use for padding.</p> required <code>length</code> <code>`int`, `str`</code> <p>Length to pad. It can be an <code>int</code>, or two string value - <code>subword</code>: the element is padded to the batch max length relative to the subwords length - <code>word</code>: the element is padded to the batch max length relative to the original word length</p> required Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def add_padding_ops(self, key: str, value: Any, length: Union[int, str]):\n    \"\"\"\n    Add padding logic to custom fields.\n    If the field is not in `self.to_tensor_inputs`, this method will add the key to it.\n\n    Args:\n        key (`str`):\n            Name of the field in the tokenizer input.\n        value (`Any`):\n            Value to use for padding.\n        length (`int`, `str`):\n            Length to pad. It can be an `int`, or two string value\n            - ``subword``: the element is padded to the batch max length relative to the subwords length\n            - ``word``: the element is padded to the batch max length relative to the original word length\n    \"\"\"\n    if key not in self.to_tensor_inputs:\n        self.to_tensor_inputs.add(key)\n    self.padding_ops[key] = partial(self.pad_sequence, value=value, length=length)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.add_special_tokens","title":"<code>add_special_tokens(special_tokens_dict)</code>","text":"<p>Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder. If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the current vocabulary).</p> <p>Parameters:</p> Name Type Description Default <code>special_tokens_dict</code> <code>`Dict`</code> <p>The dictionary containing special tokens. Keys should be in the list of predefined special attributes: [<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>, <code>additional_special_tokens</code>].</p> required <p>Returns:</p> Type Description <code>int</code> <p><code>int</code>: Number of tokens added to the vocabulary.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def add_special_tokens(\n    self, special_tokens_dict: Dict[str, Union[str, tr.AddedToken]]\n) -&gt; int:\n    \"\"\"\n    Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder.\n    If special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last\n    index of the current vocabulary).\n\n    Args:\n        special_tokens_dict (`Dict`):\n            The dictionary containing special tokens. Keys should be in\n            the list of predefined special attributes: [``bos_token``, ``eos_token``,\n            ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n            ``additional_special_tokens``].\n\n    Returns:\n        `int`: Number of tokens added to the vocabulary.\n    \"\"\"\n    return self.huggingface_tokenizer.add_special_tokens(special_tokens_dict)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.add_to_tensor_inputs","title":"<code>add_to_tensor_inputs(names)</code>","text":"<p>Add these keys to the ones that will be converted in Tensors.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>`str`, `set`</code> <p>Name of the field (or fields) to convert to tensors.</p> required <p>Returns:</p> Type Description <code>Set[str]</code> <p><code>set</code>: The set of keys that will be converted to tensors.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def add_to_tensor_inputs(self, names: Union[str, Sequence[str]]) -&gt; Set[str]:\n    \"\"\"\n    Add these keys to the ones that will be converted in Tensors.\n\n    Args:\n        names (`str`, `set`):\n            Name of the field (or fields) to convert to tensors.\n\n    Returns:\n        `set`: The set of keys that will be converted to tensors.\n    \"\"\"\n    if isinstance(names, str):\n        names = {names}\n    if not isinstance(names, set):\n        names = set(names)\n    self.to_tensor_inputs |= names\n    return self.to_tensor_inputs\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.build_scatter_offsets","title":"<code>build_scatter_offsets(model_inputs, return_tensors=True, there_is_text_pair=False)</code>","text":"<p>Build the offset tensor for the batch of inputs.</p> <p>Parameters:</p> Name Type Description Default <code>model_inputs</code> <code>`BatchEncoding`</code> <p>The inputs to the transformer model.</p> required <code>return_tensors</code> <code>`bool`, optional, defaults to `True`</code> <p>If <code>True</code>, the outputs is converted to <code>torch.Tensor</code></p> <code>True</code> <code>there_is_text_pair</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code> <code>text_pair</code> is not None.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple</code> <p><code>List[List[int]]</code> or <code>torch.Tensor</code>: The offsets of the sub-tokens.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def build_scatter_offsets(\n    self,\n    model_inputs: BatchEncoding,\n    return_tensors: bool = True,\n    there_is_text_pair: bool = False,\n) -&gt; Tuple:\n    \"\"\"\n    Build the offset tensor for the batch of inputs.\n\n    Args:\n        model_inputs (`BatchEncoding`):\n            The inputs to the transformer model.\n        return_tensors (`bool`, optional, defaults to `True`):\n            If `True`, the outputs is converted to `torch.Tensor`\n        there_is_text_pair (`bool`, optional, defaults to `False`):\n            If `True` `text_pair` is not None.\n\n    Returns:\n        `List[List[int]]` or `torch.Tensor`: The offsets of the sub-tokens.\n    \"\"\"\n    # output data structure\n    offsets = []\n    sentence_lengths = []\n    # model_inputs should be the output of the HuggingFace tokenizer\n    # it contains the word offsets to reconstruct the original tokens from the\n    # sub-tokens\n    for batch_index in range(len(model_inputs.input_ids)):\n        word_ids = model_inputs.word_ids(batch_index)\n        # it is slightly different from what we need, so here we make it compatible\n        # with our subword pooling strategy\n        # if the first token is a special token, we need to take it into account\n        if self.has_starting_token:\n            word_offsets = [0] + [\n                w + 1 if w is not None else w for w in word_ids[1:]\n            ]\n        # otherwise, we can just use word_ids as is\n        else:\n            word_offsets = word_ids\n\n        # replace first None occurrence with sep_offset\n        sep_index = word_offsets.index(None)\n\n        # here we retrieve the max offset for the sample, which will be used as SEP offset\n        # and also as padding value for the offsets\n        sep_offset_value = max([w for w in word_offsets[:sep_index] if w is not None]) + 1\n\n        word_offsets[sep_index] = sep_offset_value\n        # if there is a text pair, we need to adjust the offsets for the second text\n        if there_is_text_pair:\n            # some models have two SEP tokens in between the two texts\n            if self.has_double_sep:\n                sep_index += 1\n                sep_offset_value += 1\n                word_offsets[sep_index] = sep_offset_value\n            # keep the first offsets as is, adjust the second ones\n            word_offsets = word_offsets[: sep_index + 1] + [\n                w + sep_offset_value if w is not None else w\n                for w in word_offsets[sep_index + 1 :]\n            ]\n            # update again the sep_offset\n            sep_offset_value = max([w for w in word_offsets if w is not None]) + 1\n            # replace first None occurrence with sep_offset, it should be the last SEP\n            sep_index = word_offsets.index(None)\n            word_offsets[sep_index] = sep_offset_value\n        # keep track of the maximum offset for padding\n        offsets.append(word_offsets)\n        sentence_lengths.append(sep_offset_value + 1)\n    # replace remaining None occurrences with -1\n    # the remaining None occurrences are the padding values\n    offsets = [[o if o is not None else -1 for o in offset] for offset in offsets]\n    # if return_tensor is True, we need to convert the offsets to tensors\n    if return_tensors:\n        offsets = torch.as_tensor(offsets)\n    return offsets, sentence_lengths\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.build_sparse_offsets","title":"<code>build_sparse_offsets(offsets, bpe_mask, words_per_sentence)</code>  <code>staticmethod</code>","text":"<p>Build tensors used as info for BPE pooling, starting from the BPE offsets.</p> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>`torch.Tensor` or `List[List[int]]`</code> <p>The offsets to compute lengths from.</p> required <code>bpe_mask</code> <code>`torch.Tensor` or `List[List[int]]`</code> <p>The attention mask at BPE level.</p> required <code>words_per_sentence</code> <code>`List[int]`</code> <p>The sentence lengths, word-wise.</p> required <p>Returns:</p> Type Description <code>Mapping[str, Any]</code> <p><code>Mapping[str, Any]</code>: Tensors used to construct the sparse one which pools the</p> <code>Mapping[str, Any]</code> <p>transformer encoding word-wise.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>@staticmethod\ndef build_sparse_offsets(\n    offsets: torch.Tensor | Sequence[Sequence[int]],\n    bpe_mask: torch.Tensor | Sequence[Sequence[int]],\n    words_per_sentence: Sequence[int],\n) -&gt; Mapping[str, Any]:\n    \"\"\"Build tensors used as info for BPE pooling, starting from the BPE offsets.\n\n    Args:\n        offsets (`torch.Tensor` or `List[List[int]]`):\n            The offsets to compute lengths from.\n        bpe_mask (`torch.Tensor` or `List[List[int]]`):\n            The attention mask at BPE level.\n        words_per_sentence (`List[int]`):\n            The sentence lengths, word-wise.\n\n    Returns:\n        `Mapping[str, Any]`: Tensors used to construct the sparse one which pools the\n        transformer encoding word-wise.\n    \"\"\"\n    if not isinstance(offsets, torch.Tensor):\n        offsets: torch.Tensor = torch.as_tensor(offsets)\n    if not isinstance(bpe_mask, torch.Tensor):\n        bpe_mask: torch.Tensor = torch.as_tensor(bpe_mask)\n\n    sentence_lengths: torch.Tensor = bpe_mask.sum(dim=1)\n\n    # We want to build triplets as coordinates (document, word, bpe)\n    # We start by creating the document index for each triplet\n    document_indices = torch.arange(offsets.size(0)).repeat_interleave(\n        sentence_lengths\n    )\n    # then the word indices\n    word_indices = offsets[offsets != -1]\n    # lastly the bpe indices\n    max_range: torch.Tensor = torch.arange(bpe_mask.shape[1])\n    bpe_indices: torch.LongTensor = torch.cat(\n        [max_range[:i] for i in bpe_mask.sum(dim=1)], dim=0\n    ).long()\n\n    unique_words, word_lengths = torch.unique_consecutive(\n        offsets, return_counts=True\n    )\n    unpadded_word_lengths = word_lengths[unique_words != -1]\n\n    # and their weight to be used as multiplication factors\n    bpe_weights: torch.FloatTensor = (\n        (1 / unpadded_word_lengths).repeat_interleave(unpadded_word_lengths).float()\n    )\n\n    sparse_indices = torch.stack(\n        [document_indices, word_indices, bpe_indices], dim=0\n    )\n\n    bpe_shape = torch.Size(\n        (\n            bpe_mask.size(0),  # batch_size\n            max(words_per_sentence),  # max number of words per sentence\n            bpe_mask.size(1),  # max bpe_number in batch wrt the sentence\n        )\n    )\n\n    return dict(\n        sparse_indices=sparse_indices,\n        sparse_values=bpe_weights,\n        sparse_size=bpe_shape,\n    )\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.pad_batch","title":"<code>pad_batch(batch, max_length=None)</code>","text":"<p>Pad the batch to its maximum length or to the specified <code>max_length</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>`Dict[str, list]`</code> <p>The batch to pad.</p> required <code>max_length</code> <code>`int`</code> <p>Override maximum length of the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModelInputs</code> <p><code>Dict[str, list]</code>: The padded batch.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def pad_batch(\n    self,\n    batch: Union[ModelInputs, Dict[str, list]],\n    max_length: Optional[int] = None,\n) -&gt; ModelInputs:\n    \"\"\"\n    Pad the batch to its maximum length or to the specified `max_length`.\n\n    Args:\n        batch (`Dict[str, list]`):\n            The batch to pad.\n        max_length (`int`, optional):\n            Override maximum length of the batch.\n\n    Returns:\n        `Dict[str, list]`: The padded batch.\n    \"\"\"\n    if max_length:\n        self.subword_max_batch_len = max_length\n        self.word_max_batch_len = max_length\n    else:\n        # get maximum len inside a batch\n        self.subword_max_batch_len = max(len(x) for x in batch[\"input_ids\"])\n        self.word_max_batch_len = max(x for x in batch[\"sentence_lengths\"])\n\n    for key in batch:\n        if key in self.padding_ops:\n            batch[key] = [self.padding_ops[key](b) for b in batch[key]]\n\n    return ModelInputs(batch)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.pad_sequence","title":"<code>pad_sequence(sequence, value, length='subword', pad_to_left=False)</code>","text":"<p>Pad the input to the specified length with the given value.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>`List`, `torch.Tensor`</code> <p>Element to pad, it can be either a <code>List</code> or a <code>torch.Tensor</code>.</p> required <code>value</code> <code>`int`</code> <p>Value to use as padding.</p> required <code>length</code> <code>`int`, `str`, optional, defaults to `subword`</code> <p>Length after pad.</p> <code>'subword'</code> <code>pad_to_left</code> <code>`bool`, optional, defaults to `False`</code> <p>If <code>True</code>, pads to the left, right otherwise.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[List, Tensor]</code> <p><code>List</code>, <code>torch.Tensor</code>: The padded sequence.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def pad_sequence(\n    self,\n    sequence: Union[List, torch.Tensor],\n    value: int,\n    length: Union[int, str] = \"subword\",\n    pad_to_left: bool = False,\n) -&gt; Union[List, torch.Tensor]:\n    \"\"\"\n    Pad the input to the specified length with the given value.\n\n    Args:\n        sequence (`List`, `torch.Tensor`):\n            Element to pad, it can be either a `List` or a `torch.Tensor`.\n        value (`int`):\n            Value to use as padding.\n        length (`int`, `str`, optional, defaults to `subword`):\n            Length after pad.\n        pad_to_left (`bool`, optional, defaults to `False`):\n            If `True`, pads to the left, right otherwise.\n\n    Returns:\n        `List`, `torch.Tensor`: The padded sequence.\n    \"\"\"\n    if length == \"subword\":\n        length = self.subword_max_batch_len\n    elif length == \"word\":\n        length = self.word_max_batch_len\n    else:\n        if not isinstance(length, int):\n            raise ValueError(\n                f\"`length` must be an `int`, `subword` or `word`. Current value is `{length}`\"\n            )\n    padding = [value] * abs(length - len(sequence))\n    if isinstance(sequence, torch.Tensor):\n        if len(sequence.shape) &gt; 1:\n            raise ValueError(\n                f\"Sequence tensor must be 1D. Current shape is `{len(sequence.shape)}`\"\n            )\n        padding = torch.as_tensor(padding)\n    if pad_to_left:\n        if isinstance(sequence, torch.Tensor):\n            return torch.cat((padding, sequence), -1)\n        return padding + sequence\n    if isinstance(sequence, torch.Tensor):\n        return torch.cat((sequence, padding), -1)\n    return sequence + padding\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/tokenizer/#transformers_embedder.tokenizer.Tokenizer.to_tensor","title":"<code>to_tensor(batch)</code>","text":"<p>Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in <code>self.to_tensor_inputs</code>. By default, only the standard model inputs are converted. Use <code>self.add_to_tensor_inputs</code> to add custom fields.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>`List[dict]`, `dict`</code> <p>Batch in input.</p> required <p>Returns:</p> Type Description <code>ModelInputs</code> <p><code>ModelInputs</code>: The batch as tensor.</p> Source code in <code>transformers_embedder/tokenizer.py</code> <pre><code>def to_tensor(self, batch: Union[ModelInputs, List[dict], dict]) -&gt; ModelInputs:\n    \"\"\"\n    Return the batch in input as Pytorch tensors. The fields that are converted in tensors are in\n    `self.to_tensor_inputs`. By default, only the standard model inputs are converted. Use\n    `self.add_to_tensor_inputs` to add custom fields.\n\n    Args:\n        batch (`List[dict]`, `dict`):\n            Batch in input.\n\n    Returns:\n        `ModelInputs`: The batch as tensor.\n    \"\"\"\n    # convert to tensor\n    batch = {\n        k: torch.as_tensor(v)\n        if k in self.to_tensor_inputs and not isinstance(v, torch.Tensor)\n        else v\n        for k, v in batch.items()\n    }\n    return ModelInputs(batch)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/utils/","title":"Utils","text":""},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/utils/#transformers_embedder.utils.get_logger","title":"<code>get_logger(name)</code>","text":"<p>Return the logger of the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str`</code> <p>The name of the logger.</p> required <p>Returns:</p> Type Description <code>Logger</code> <p><code>logging.Logger</code>: The logger of the given name.</p> Source code in <code>transformers_embedder/utils.py</code> <pre><code>def get_logger(name: str) -&gt; logging.Logger:\n    \"\"\"\n    Return the logger of the given name.\n\n    Args:\n        name (`str`): The name of the logger.\n\n    Returns:\n        `logging.Logger`: The logger of the given name.\n    \"\"\"\n    return logging.getLogger(name)\n</code></pre>"},{"location":"/home/runner/work/transformers-embedder/transformers-embedder/references/utils/#transformers_embedder.utils.is_torch_available","title":"<code>is_torch_available()</code>","text":"<p>Check if PyTorch is available.</p> Source code in <code>transformers_embedder/utils.py</code> <pre><code>def is_torch_available():\n    \"\"\"Check if PyTorch is available.\"\"\"\n    return _torch_available\n</code></pre>"}]}